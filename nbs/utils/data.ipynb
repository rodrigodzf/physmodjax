{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from typing import Optional, List, Union\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from einops import rearrange\n",
    "from collections.abc import Iterable\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def standardize(x, mean, std, only_scale=True):\n",
    "    return (x - mean) / std if not only_scale else x / std\n",
    "\n",
    "\n",
    "def unstandardize(x, mean, std, only_scale=True):\n",
    "    return x * std + mean if not only_scale else x * std\n",
    "\n",
    "\n",
    "# load the data\n",
    "def load_as_big_array(files):\n",
    "    return np.stack([np.load(file) for file in files], axis=0)\n",
    "\n",
    "\n",
    "def reshape_array(array):\n",
    "    n_runs, n_timesteps, n_gridpoints, n_channels = array.shape\n",
    "    return np.reshape(array, (n_runs * n_timesteps, n_gridpoints, n_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def create_grid(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    min_val: float = -1.0,\n",
    "    max_val: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a grid of size (height, width) with values between min_val and max_val inclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    y, x = jnp.meshgrid(\n",
    "        jnp.linspace(-min_val, max_val, height),\n",
    "        jnp.linspace(-min_val, max_val, width),\n",
    "    )\n",
    "\n",
    "    grid = jnp.stack([x, y], axis=-1)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def hankel_matrix(\n",
    "    x,  # input array with shape (B, grid_size, time_steps, channels)\n",
    "    depth: int = 2,  # repeats the array on the first axis n times\n",
    ") -> (\n",
    "    np.ndarray\n",
    "):  # returns an array with shape (B, grid_size * depth, time_steps - (d-1), channels)\n",
    "\n",
    "    x = x.transpose(0, 2, 1, 3)\n",
    "    d = depth - 1\n",
    "    b = sliding_window_view(x, window_shape=(x.shape[1], x.shape[2] - d), axis=(1, 2))\n",
    "    b = b.transpose(0, 1, 2, 4, 5, 3)\n",
    "    b = b.reshape(x.shape[0], -1, x.shape[2] - d, x.shape[-1])\n",
    "    return b.transpose(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "a = np.arange(20).reshape(5, 4)\n",
    "a = np.stack([a, a, a], axis=0)\n",
    "a = np.stack([a, a*2], axis=-1)\n",
    "d = 2\n",
    "\n",
    "b = hankel_matrix(a, depth=2)\n",
    "assert b.shape == (a.shape[0], a.shape[1]- (d-1), a.shape[2]*d, a.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import jax\n",
    "from timeit import default_timer as timer\n",
    "from functools import partial\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def slice_tensor_multi(\n",
    "    key: jax.random.PRNGKey,\n",
    "    x: jnp.ndarray,  # input array with shape (T, ...)\n",
    "    num_slices: int,  # number of slices to take\n",
    "    num_input: int,  # number of time steps as input\n",
    "    num_target: int,  # number of time steps as target\n",
    "    split_mode: str = \"no_overlap\",  # no_split, overlap, no_overlap\n",
    "):\n",
    "    \"\"\"\n",
    "    Many random slices of the tensor along the time axis\n",
    "    Warning: Here the time axis is the first axis of the tensor\n",
    "    \"\"\"\n",
    "    total_samples = x.shape[0]\n",
    "    num_steps = num_input + num_target\n",
    "    last_start_sample = total_samples - num_steps\n",
    "    indices = jax.random.choice(key, last_start_sample, shape=(num_slices,))\n",
    "\n",
    "    def slice_tensor(start_index):\n",
    "        if split_mode == \"no_overlap\":\n",
    "            input_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index, num_input, axis=0\n",
    "            )\n",
    "            target_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index + num_input, num_target, axis=0\n",
    "            )\n",
    "            return input_slices, target_slices\n",
    "        elif split_mode == \"overlap\":\n",
    "            input_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index, num_steps - 1, axis=0\n",
    "            )\n",
    "            target_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index + 1, num_steps - 1, axis=0\n",
    "            )\n",
    "            return input_slices, target_slices\n",
    "        elif split_mode == \"no_split\":\n",
    "            slices = jax.lax.dynamic_slice_in_dim(x, start_index, num_steps, axis=0)\n",
    "            return slices\n",
    "\n",
    "    slices = jax.vmap(slice_tensor)(indices)\n",
    "\n",
    "    return slices\n",
    "\n",
    "\n",
    "def slice_tensor_single(\n",
    "    key: jax.random.PRNGKey,\n",
    "    x: jnp.ndarray,  # input array with shape (B, T, ...)\n",
    "    num_input,  # number of time steps to slice\n",
    "    num_target,  # number of time steps to predict\n",
    "    split_mode: str = \"no_overlap\",  # no_split, overlap, no_overlap\n",
    "):\n",
    "    # random slices\n",
    "    total_samples = x.shape[1]\n",
    "    num_steps = num_input + num_target\n",
    "    last_start_sample = total_samples - num_steps\n",
    "\n",
    "    start_index = jax.random.choice(\n",
    "        key,\n",
    "        last_start_sample,\n",
    "    )\n",
    "\n",
    "    def slice_tensor(start_index):\n",
    "        if split_mode == \"no_overlap\":\n",
    "            input_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index, num_input, axis=1\n",
    "            )\n",
    "            target_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index + num_input, num_target, axis=1\n",
    "            )\n",
    "            return input_slices, target_slices\n",
    "        elif split_mode == \"overlap\":\n",
    "            input_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index, num_steps - 1, axis=1\n",
    "            )\n",
    "            target_slices = jax.lax.dynamic_slice_in_dim(\n",
    "                x, start_index + 1, num_steps - 1, axis=1\n",
    "            )\n",
    "            return input_slices, target_slices\n",
    "        elif split_mode == \"no_split\":\n",
    "            slices = jax.lax.dynamic_slice_in_dim(x, start_index, num_steps, axis=1)\n",
    "            return slices\n",
    "\n",
    "    # get the slices of the tensor\n",
    "    slices = slice_tensor(start_index)\n",
    "\n",
    "    return slices\n",
    "\n",
    "\n",
    "def split_xy(\n",
    "    data,\n",
    "    num_input,\n",
    "    num_target,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split the data into input and target\n",
    "    \"\"\"\n",
    "    x = jax.lax.slice_in_dim(data, 0, num_input, axis=1)\n",
    "    y = jax.lax.slice_in_dim(data, num_input, data.shape[1], axis=1)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"mode\", \"num_input\", \"num_target\", \"batch_size\"))\n",
    "def select_slices(\n",
    "    key: jax.random.PRNGKey,\n",
    "    idx: jnp.ndarray,  # batch indices\n",
    "    data: jnp.ndarray,  # input array with shape (B, T, ...)\n",
    "    indices: jnp.ndarray,  # indices of the batches\n",
    "    num_input: int,  # number of time steps to slice\n",
    "    num_target: int,  # number of time steps to predict\n",
    "    mode: str,  # blocks, sequential, passthrough\n",
    "    batch_size: int,  # number of slices to take in multi_block mode\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "\n",
    "    batch_data = data[indices[idx]]\n",
    "    if mode == \"single_random_no_overlap\":\n",
    "        return slice_tensor_single(key, batch_data, num_input, num_target, \"no_overlap\")\n",
    "    elif mode == \"single_random_overlap\":\n",
    "        return slice_tensor_single(key, batch_data, num_input, num_target, \"overlap\")\n",
    "    elif mode == \"single_random_no_split\":\n",
    "        return slice_tensor_single(key, batch_data, num_input, num_target, \"no_split\")\n",
    "    elif mode == \"many_random_no_overlap\":\n",
    "        return slice_tensor_multi(\n",
    "            key, batch_data, batch_size, num_input, num_target, \"no_overlap\"\n",
    "        )\n",
    "    elif mode == \"many_random_overlap\":\n",
    "        return slice_tensor_multi(\n",
    "            key, batch_data, batch_size, num_input, num_target, \"overlap\"\n",
    "        )\n",
    "    elif mode == \"many_random_no_split\":\n",
    "        return slice_tensor_multi(\n",
    "            key, batch_data, batch_size, num_input, num_target, \"no_split\"\n",
    "        )\n",
    "    elif mode == \"split\":\n",
    "        return split_xy(batch_data, num_input, num_target)\n",
    "    elif mode == \"passthrough\":\n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "B, T, W, C = 16, 4000, 40, 2\n",
    "x = jnp.ones((B, T, W, C))\n",
    "\n",
    "num_input = 1\n",
    "num_target = 199\n",
    "\n",
    "x, y = select_slices(\n",
    "    key=jax.random.PRNGKey(65),\n",
    "    idx=0,\n",
    "    data=x,\n",
    "    indices=jnp.arange(B),\n",
    "    num_input=num_input,\n",
    "    num_target=num_target,\n",
    "    mode=\"many_random_no_overlap\",\n",
    "    batch_size=B,\n",
    ")\n",
    "assert x.shape == (B, num_input, W, C)\n",
    "assert y.shape == (B, num_target, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "x = jnp.ones((B, T, W, C))\n",
    "\n",
    "x, y = select_slices(\n",
    "    key=jax.random.PRNGKey(65),\n",
    "    idx=0,\n",
    "    data=x,\n",
    "    indices=jnp.arange(B),\n",
    "    num_input=1,\n",
    "    num_target=199,\n",
    "    mode=\"many_random_overlap\",\n",
    "    batch_size=B,\n",
    ")\n",
    "assert x.shape == (B, 199, W, C)\n",
    "assert y.shape == (B, 199, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "x = jnp.ones((B, T, W, C))\n",
    "\n",
    "\n",
    "x, y = select_slices(\n",
    "    key=jax.random.PRNGKey(65),\n",
    "    idx=jnp.arange(B),\n",
    "    data=x,\n",
    "    indices=jnp.arange(B),\n",
    "    num_input=num_input,\n",
    "    num_target=num_target,\n",
    "    mode=\"single_random_no_overlap\",\n",
    "    batch_size=None,\n",
    ")\n",
    "assert x.shape == (B, num_input, W, C)\n",
    "assert y.shape == (B, num_target, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class JaxDataloader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        num_input: int = -1,  # length of the input segment\n",
    "        num_target: int = -1,  # length of the target segment\n",
    "        batch_size: int = 16,  # batch size\n",
    "        shuffle: bool = True,  # shuffle the data\n",
    "        drop_last: bool = True,  # drop the last batch if it's smaller than batch_size\n",
    "        key: Optional[jax.random.PRNGKey] = jax.random.PRNGKey(0),  # random key\n",
    "        mode: str = \"passthrough\",  # mode of the dataloader\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.num_input = num_input\n",
    "        self.num_target = num_target\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.key = key\n",
    "        self.mode = mode\n",
    "        self.data_fits_batch_size = self.data.shape[0] % batch_size == 0\n",
    "        if self.mode in [\n",
    "            \"many_random_no_overlap\",\n",
    "            \"many_random_overlap\",\n",
    "            \"many_random_no_split\",\n",
    "        ]:\n",
    "            # in many_random mode the batch size is the number of slices within a single trajectory\n",
    "            self.indices = jnp.arange(data.shape[0])\n",
    "        else:\n",
    "            if not self.data_fits_batch_size:\n",
    "                logging.info(\n",
    "                    f\"Warning: The data size {self.data.shape[0]} is not divisible by the batch size {batch_size}.\"\n",
    "                )\n",
    "                # Adjust batch size to fit data\n",
    "                self.batch_size = self.data.shape[0] // (\n",
    "                    self.data.shape[0] // batch_size + 1\n",
    "                )\n",
    "                logging.info(f\"Setting the batch size to {self.batch_size}\")\n",
    "                # Update indices\n",
    "                self.indices = jnp.arange(data.shape[0]).reshape(-1, self.batch_size)\n",
    "            else:\n",
    "                self.indices = jnp.arange(data.shape[0]).reshape(-1, batch_size)\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset the dataloader for a new iteration over the data.\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.key = jax.random.split(self.key, 1)[0]\n",
    "            self.indices = jax.random.permutation(self.key, self.indices)\n",
    "\n",
    "        self.idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def get_slices(self):\n",
    "        # Increment the idx and fold it into the key for randomness\n",
    "        self.key = jax.random.fold_in(self.key, self.idx)\n",
    "\n",
    "        # Call the select_slices function with common parameters\n",
    "        result = select_slices(\n",
    "            self.key,\n",
    "            self.idx,\n",
    "            self.data,\n",
    "            self.indices,\n",
    "            self.num_input,\n",
    "            self.num_target,\n",
    "            self.mode,\n",
    "            self.batch_size,\n",
    "        )\n",
    "\n",
    "        self.idx += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __next__(self) -> Union[Tuple[jnp.ndarray, jnp.ndarray], jnp.ndarray]:\n",
    "        if self.idx >= len(self.indices):\n",
    "            # Reset the iterator and raise StopIteration\n",
    "            self._reset()\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            if self.mode not in [\n",
    "                \"many_random_no_overlap\",\n",
    "                \"many_random_overlap\",\n",
    "                \"many_random_no_split\",\n",
    "            ]:\n",
    "                if self.drop_last and len(self.data) - self.idx < self.batch_size:\n",
    "                    # Reset and stop if we're dropping the last batch and it's too small\n",
    "                    self._reset()\n",
    "                    raise StopIteration\n",
    "\n",
    "            return self.get_slices()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            # Exclude the last batch if it has fewer than batch_size elements\n",
    "            return len(self.data) // self.batch_size\n",
    "        else:\n",
    "            # Include the last batch regardless of its size\n",
    "            return int(np.ceil(len(self.data) / self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def make_rolling_windows(\n",
    "    x: np.ndarray,  # (B, T, ...)\n",
    "    window_size: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate rolling windows of size window_size over the time axis of x.\n",
    "    Warning: This might generate a large amount of data if the input is large.\n",
    "    \"\"\"\n",
    "    x = rearrange(x, \"b t ... -> b ... t\")\n",
    "    x = sliding_window_view(x, window_shape=(window_size), axis=(-1,))\n",
    "    x = rearrange(x, \"b ... n w -> (b n) w ...\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "B, T, H, W, C = 5, 64, 40, 40, 2\n",
    "win = 16\n",
    "dummy = np.ones((B, T, H, W, C))\n",
    "dummy = make_rolling_windows(dummy, win)\n",
    "assert dummy.shape == (B * (T-win+1), win, H, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    mapped_data,\n",
    "    split: List[float],\n",
    "    extract_channels: List[int],\n",
    "):\n",
    "    n_train, n_val, n_test = [\n",
    "        int(fraction * mapped_data.shape[0]) for fraction in split\n",
    "    ]\n",
    "    \n",
    "    n_total = n_train + n_val + n_test\n",
    "    if n_total < mapped_data.shape[0]:\n",
    "        n_rest = mapped_data.shape[0] - n_total\n",
    "        n_train += n_rest\n",
    "        \n",
    "    assert (\n",
    "        n_train + n_val + n_test == mapped_data.shape[0]\n",
    "    ), \"Split fractions do not sum up correctly\"\n",
    "\n",
    "    train = mapped_data[:n_train]\n",
    "    val = mapped_data[n_train : n_train + n_val]\n",
    "    test = mapped_data[n_train + n_val :]\n",
    "\n",
    "    if extract_channels:\n",
    "        train = train[..., extract_channels]\n",
    "        val = val[..., extract_channels]\n",
    "        test = test[..., extract_channels]\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def is_list_like(obj):\n",
    "    return isinstance(obj, Iterable) and not isinstance(obj, (str, bytes, int))\n",
    "\n",
    "\n",
    "class DirectoryDataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_array: Union[str, List[str]],\n",
    "        split: List[float] = [0.8, 0.2, 0.0],  # train, val, test\n",
    "        batch_size: int = 1,  # batch size per device (time steps in parallel mode)\n",
    "        extract_channels: List[int] = [],  # extract only these channels (u, v)\n",
    "        num_steps_train: Optional[\n",
    "            Union[int, List[int]]\n",
    "        ] = None,  # number of time steps to use for training\n",
    "        num_steps_val: Optional[\n",
    "            Union[int, List[int]]\n",
    "        ] = None,  # number of time steps to use for validation\n",
    "        standardize_dataset: bool = False,  # standardize the dataset per channel\n",
    "        mean: Optional[float] = None,  # mean for standardization\n",
    "        std: Optional[float] = None,  # std for standardization\n",
    "        mode: str = \"passthrough\",  # mode of the dataloader\n",
    "        windowed: bool = False,  # slice the data in a windowedd way\n",
    "        hankelize: int = 0,  # hankelize the data with depth (0 = no hankelization)\n",
    "        shuffle_train: bool = True,  # shuffle the training data\n",
    "        cache: bool = False,  # cache the data\n",
    "        rolling_windows: bool = False,  # generate rolling windows\n",
    "        total_num_train: Optional[int] = None,  # total number of training samples\n",
    "        total_num_val: Optional[int] = None,  # total number of validation samples\n",
    "        total_num_test: Optional[int] = None,  # total number of test samples\n",
    "    ):\n",
    "        assert mode in [\n",
    "            \"single_random_no_overlap\",\n",
    "            \"single_random_overlap\",\n",
    "            \"single_random_no_split\",\n",
    "            \"many_random_no_overlap\",\n",
    "            \"many_random_overlap\",\n",
    "            \"many_random_no_split\",\n",
    "            \"split\",\n",
    "            \"passthrough\",\n",
    "        ], \"Invalid mode\"\n",
    "        self.mode = mode\n",
    "        self.num_steps_train = num_steps_train\n",
    "        self.num_steps_val = num_steps_val\n",
    "        self.train_batch_size = batch_size\n",
    "        self.val_batch_size = batch_size\n",
    "\n",
    "        assert Path(data_array).exists() or all(\n",
    "            [Path(d).exists() for d in data_array]\n",
    "        ), \"The data array does not exist\"\n",
    "\n",
    "        # if we have a list of directories, we will concatenate the data\n",
    "        # else we will assume that the directory contains the data\n",
    "        if is_list_like(data_array):\n",
    "            # TODO: this will not work as we need to create a third memmap file\n",
    "            raise NotImplementedError(\n",
    "                \"Concatenating data from different arrays is not supported yet\"\n",
    "            )\n",
    "        else:\n",
    "            mapped_data = np.load(data_array, mmap_mode=\"r\")\n",
    "\n",
    "        logging.info(f\"Found {mapped_data.shape[0]} trajectories\")\n",
    "\n",
    "        # split\n",
    "        if extract_channels is None:\n",
    "            extract_channels = list(range(mapped_data.shape[-1]))\n",
    "        train_array, val_array, test_array = split_data(\n",
    "            mapped_data,\n",
    "            split,\n",
    "            extract_channels,\n",
    "        )\n",
    "\n",
    "        # slice only the number of time steps\n",
    "        if total_num_train is not None:\n",
    "            train_array = train_array[:, :total_num_train, ...]\n",
    "        if total_num_val is not None:\n",
    "            val_array = val_array[:, :total_num_val, ...]\n",
    "        if total_num_test is not None:\n",
    "            test_array = test_array[:, :total_num_test, ...]\n",
    "\n",
    "        self.num_steps_input_train, self.num_steps_target_train = self._parse_num_steps(\n",
    "            num_steps_train,\n",
    "        )\n",
    "\n",
    "        self.num_steps_input_val, self.num_steps_target_val = self._parse_num_steps(\n",
    "            num_steps_val\n",
    "        )\n",
    "\n",
    "        if hankelize != 0:\n",
    "            raise NotImplementedError(\"Hankelization is not supported yet\")\n",
    "\n",
    "        if windowed:\n",
    "            train_array = self._windowed_data(\n",
    "                train_array, self.num_steps_input_train + self.num_steps_target_train\n",
    "            )\n",
    "            val_array = self._windowed_data(\n",
    "                val_array, self.num_steps_input_val + self.num_steps_target_val\n",
    "            )\n",
    "\n",
    "        if rolling_windows:\n",
    "            train_array = make_rolling_windows(train_array, window_size=num_steps_train)\n",
    "            val_array = make_rolling_windows(val_array, window_size=num_steps_val)\n",
    "\n",
    "        # TODO: there is no reliable way to know the shape of the numpy array\n",
    "        # we save the shape of the first entry\n",
    "        self.data_shape = train_array[0].shape\n",
    "\n",
    "        # after slicing to cache the data on the GPU or map some operation\n",
    "        if cache:\n",
    "            timer_start = timer()\n",
    "            train_array = jax.device_put(train_array).block_until_ready()\n",
    "            val_array = jax.device_put(val_array).block_until_ready()\n",
    "            logging.info(f\"Data cached in {timer() - timer_start} seconds\")\n",
    "\n",
    "        if standardize_dataset:\n",
    "            # standardize the output per channel\n",
    "            # the statistics are computed on the training set\n",
    "            train_array, mean, std = self._standardize_data(train_array, mean, std)\n",
    "            val_array, *_ = self._standardize_data(val_array, mean, std)\n",
    "            test_array, *_ = self._standardize_data(test_array, mean, std)\n",
    "\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            logging.info(\n",
    "                f\"The mean and std of the output are {self.mean} and {self.std}, you should save this for later\"\n",
    "            )\n",
    "\n",
    "        self.train_dataloader = JaxDataloader(\n",
    "            train_array,\n",
    "            num_input=self.num_steps_input_train,\n",
    "            num_target=self.num_steps_target_train,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_train,\n",
    "            drop_last=True,\n",
    "            mode=self.mode,\n",
    "        )\n",
    "\n",
    "        self.val_dataloader = JaxDataloader(\n",
    "            val_array,\n",
    "            num_input=self.num_steps_input_val,\n",
    "            num_target=self.num_steps_target_val,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            mode=self.mode,\n",
    "        )\n",
    "\n",
    "        self.test_dataloader = JaxDataloader(\n",
    "            test_array,\n",
    "            batch_size=1,  # we will handle the batch size in the slice function\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            mode=\"passthrough\",  # always passthrough\n",
    "        )\n",
    "\n",
    "        logging.info(\n",
    "            f\"Using {self.num_steps_input_train} input and {self.num_steps_target_train} target steps for training in mode {mode}\"\n",
    "        )\n",
    "\n",
    "    def _standardize_data(self, data_array, mean, std):\n",
    "        if mean is None or std is None:\n",
    "            if (\n",
    "                len(data_array.shape) == 4\n",
    "            ):  # (batch_size, time_steps, grid_size, channels)\n",
    "                mean = jnp.mean(data_array, axis=(0, 1, 2))\n",
    "                std = jnp.std(data_array, axis=(0, 1, 2))\n",
    "            elif len(data_array.shape) == 5:  # (batch_size, time_steps, H, W, channels)\n",
    "                mean = jnp.mean(data_array, axis=(0, 1, 2, 3))\n",
    "                std = jnp.std(data_array, axis=(0, 1, 2, 3))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"The input array has an incorrect number of dimensions\"\n",
    "                )\n",
    "        data_array = standardize(data_array, mean, std)\n",
    "\n",
    "        return data_array, mean, std\n",
    "\n",
    "    def _windowed_data(\n",
    "        self,\n",
    "        data_array,\n",
    "        num_steps,\n",
    "    ):\n",
    "        assert (\n",
    "            data_array.shape[1] % num_steps == 0\n",
    "        ), \"The number of time steps is not divisible by the slice size\"\n",
    "        data_array = data_array.reshape(-1, num_steps, *data_array.shape[2:])\n",
    "        return data_array\n",
    "\n",
    "    def _parse_num_steps(\n",
    "        self,\n",
    "        num_steps,\n",
    "    ):\n",
    "        if is_list_like(num_steps):\n",
    "            num_steps_input, num_steps_target = num_steps\n",
    "        else:\n",
    "            num_steps_input = num_steps\n",
    "            num_steps_target = num_steps\n",
    "        return num_steps_input, num_steps_target\n",
    "\n",
    "    def get_info(self):\n",
    "        # return the correct shape of the data after slicing\n",
    "        if self.mode in [\"single_random_overlap\", \"many_random_overlap\"]:\n",
    "            return [\n",
    "                self.num_steps_input_train + self.num_steps_target_train - 1,\n",
    "                *self.data_shape[1:],\n",
    "            ]\n",
    "        else:\n",
    "            return [self.num_steps_input_train, *self.data_shape[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "# generate and save the data\n",
    "num_samples = 100\n",
    "num_timesteps = 120\n",
    "grid_size = 20\n",
    "channels = 2\n",
    "\n",
    "data = np.random.randn(num_samples, num_timesteps, grid_size, channels)\n",
    "np.save(\"/tmp/data.npy\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "datamodule = DirectoryDataModule(\n",
    "    data_array=\"/tmp/data.npy\",\n",
    "    split=[0.8, 0.1, 0.1],\n",
    "    batch_size=batch_size,\n",
    "    extract_channels=[0, 1],\n",
    "    num_steps_train=[1, 19],\n",
    "    num_steps_val=[3, 17],\n",
    "    cache=True,\n",
    "    standardize_dataset=True,\n",
    "    mode=\"many_random_no_overlap\",\n",
    "    windowed=False,\n",
    "    hankelize=0,\n",
    ")\n",
    "\n",
    "train_x, train_y = next(iter(datamodule.train_dataloader))\n",
    "assert train_x.shape == (batch_size, 1, grid_size, channels)\n",
    "assert train_y.shape == (batch_size, 19, grid_size, channels)\n",
    "\n",
    "val_x, val_y = next(iter(datamodule.val_dataloader))\n",
    "assert val_x.shape == (batch_size, 3, grid_size, channels)\n",
    "assert val_y.shape == (batch_size, 17, grid_size, channels)\n",
    "\n",
    "test_data = next(iter(datamodule.test_dataloader))\n",
    "assert test_data.shape == (1, num_timesteps, grid_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "batch_size = 2\n",
    "datamodule = DirectoryDataModule(\n",
    "    data_array=\"/tmp/data.npy\",\n",
    "    split=[0.8, 0.1, 0.1],\n",
    "    batch_size=batch_size,\n",
    "    extract_channels=[0, 1],\n",
    "    cache=True,\n",
    "    standardize_dataset=True,\n",
    "    mode=\"passthrough\",\n",
    "    windowed=False,\n",
    ")\n",
    "\n",
    "train_x = next(iter(datamodule.train_dataloader))\n",
    "assert train_x.shape == (batch_size, num_timesteps, grid_size, channels)\n",
    "\n",
    "val_x = next(iter(datamodule.val_dataloader))\n",
    "assert val_x.shape == (batch_size, num_timesteps, grid_size, channels)\n",
    "\n",
    "test_data = next(iter(datamodule.test_dataloader))\n",
    "assert test_data.shape == (1, num_timesteps, grid_size, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "batch_size = 2\n",
    "num_steps_train = [1, 59]\n",
    "num_steps_val = [1, 59]\n",
    "datamodule = DirectoryDataModule(\n",
    "    data_array=\"/tmp/data.npy\",\n",
    "    split=[0.8, 0.1, 0.1],\n",
    "    batch_size=batch_size,\n",
    "    extract_channels=[0, 1],\n",
    "    num_steps_train=num_steps_train,\n",
    "    num_steps_val=num_steps_val,\n",
    "    cache=True,\n",
    "    standardize_dataset=True,\n",
    "    mode=\"passthrough\",\n",
    "    windowed=True,\n",
    ")\n",
    "\n",
    "train_x = next(iter(datamodule.train_dataloader))\n",
    "assert train_x.shape == (batch_size, sum(num_steps_train), grid_size, channels)\n",
    "\n",
    "val_x = next(iter(datamodule.val_dataloader))\n",
    "assert val_x.shape == (batch_size, sum(num_steps_val), grid_size, channels)\n",
    "\n",
    "test_data = next(iter(datamodule.test_dataloader))\n",
    "assert test_data.shape == (1, num_timesteps, grid_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "batch_size = 2\n",
    "num_steps_train = 50\n",
    "num_steps_val = 50\n",
    "datamodule = DirectoryDataModule(\n",
    "    data_array=\"/tmp/data.npy\",\n",
    "    split=[0.8, 0.1, 0.1],\n",
    "    batch_size=batch_size,\n",
    "    extract_channels=[0, 1],\n",
    "    num_steps_train=[1, 49],\n",
    "    num_steps_val=[1, 49],\n",
    "    cache=True,\n",
    "    standardize_dataset=True,\n",
    "    mode=\"many_random_no_split\",\n",
    "    windowed=False,\n",
    ")\n",
    "\n",
    "train_x = next(iter(datamodule.train_dataloader))\n",
    "assert train_x.shape == (batch_size, num_steps_train, grid_size, channels)\n",
    "\n",
    "val_x = next(iter(datamodule.val_dataloader))\n",
    "assert val_x.shape == (batch_size, num_steps_val, grid_size, channels)\n",
    "\n",
    "test_data = next(iter(datamodule.test_dataloader))\n",
    "assert test_data.shape == (1, num_timesteps, grid_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "batch_size = 16\n",
    "num_steps_train = 50\n",
    "num_steps_val = 50\n",
    "datamodule = DirectoryDataModule(\n",
    "    data_array=\"/tmp/data.npy\",\n",
    "    split=[0.8, 0.1, 0.1],\n",
    "    batch_size=batch_size,\n",
    "    extract_channels=[0, 1],\n",
    "    num_steps_train=[1, 49],\n",
    "    num_steps_val=[1, 49],\n",
    "    cache=True,\n",
    "    standardize_dataset=True,\n",
    "    mode=\"single_random_no_split\",\n",
    "    windowed=False,\n",
    ")\n",
    "\n",
    "train_x = next(iter(datamodule.train_dataloader))\n",
    "assert train_x.shape == (batch_size, num_steps_train, grid_size, channels)\n",
    "\n",
    "val_x = next(iter(datamodule.val_dataloader))\n",
    "assert val_x.shape == (10, num_steps_val, grid_size, channels)\n",
    "\n",
    "test_data = next(iter(datamodule.test_dataloader))\n",
    "assert test_data.shape == (1, num_timesteps, grid_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "batch_size = 16\n",
    "num_steps_train = [1, 59]\n",
    "num_steps_val = [1, 59]\n",
    "datamodule = DirectoryDataModule(\n",
    "    data_array=\"/tmp/data.npy\",\n",
    "    split=[0.8, 0.1, 0.1],\n",
    "    batch_size=batch_size,\n",
    "    extract_channels=[0, 1],\n",
    "    num_steps_train=num_steps_train,\n",
    "    num_steps_val=num_steps_val,\n",
    "    cache=True,\n",
    "    standardize_dataset=True,\n",
    "    mode=\"split\",\n",
    "    windowed=True,\n",
    ")\n",
    "\n",
    "train_x, train_y = next(iter(datamodule.train_dataloader))\n",
    "assert train_x.shape == (batch_size, num_steps_train[0], grid_size, channels)\n",
    "\n",
    "val_x, val_y = next(iter(datamodule.val_dataloader))\n",
    "assert val_x.shape == (10, num_steps_val[0], grid_size, channels)\n",
    "\n",
    "test_data = next(iter(datamodule.test_dataloader))\n",
    "assert test_data.shape == (1, num_timesteps, grid_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29 ms ± 156 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "def iterate_over_dataloader():\n",
    "    for x in datamodule.train_dataloader:\n",
    "        pass\n",
    "\n",
    "%timeit iterate_over_dataloader()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
