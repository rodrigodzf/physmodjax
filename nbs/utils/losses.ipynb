{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import custom_vjp\n",
    "from typing import Optional, Tuple\n",
    "import optax\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def to_db(\n",
    "    x: jnp.ndarray,\n",
    "    eps: float = 1e-10,\n",
    "):\n",
    "    return 20 * jnp.log10(x + eps)\n",
    "\n",
    "\n",
    "def log_mag(\n",
    "    x: jnp.ndarray,\n",
    "    eps: float = 1e-10,\n",
    "):\n",
    "    return jnp.log(jnp.abs(x) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def db_mag_loss(\n",
    "    predicted: jnp.ndarray,  # Complex-valued FFT of the predicted signal\n",
    "    target: jnp.ndarray,  # Complex-valued FFT of the target signal\n",
    "    eps: float = 1e-10,  # Small constant to avoid log(0)\n",
    "    distance: str = \"l1\",  # Distance metric: 'l1' or 'l2'\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the mean L1 or L2 loss between the decibel magnitudes of two FFT signals.\n",
    "\n",
    "    :param predicted: FFT of the predicted signal.\n",
    "    :param target: FFT of the target signal.\n",
    "    :param epsilon: Small constant for numerical stability in log computation.\n",
    "    :param distance_metric: Type of distance metric ('l1' or 'l2').\n",
    "    :return: Mean L1 or L2 loss in decibel magnitude.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to decibel magnitude\n",
    "    pred_db = to_db(jnp.abs(predicted), eps)\n",
    "    target_db = to_db(jnp.abs(target), eps)\n",
    "\n",
    "    # Compute loss based on the specified distance metric\n",
    "    if distance == \"l1\":\n",
    "        return jnp.mean(jnp.abs(pred_db - target_db))\n",
    "    elif distance == \"l2\":\n",
    "        return jnp.mean((pred_db - target_db) ** 2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid distance metric. Choose 'l1' or 'l2'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The spectral log magnitude loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N}|| \\log({Y}) - \\log({\\hat{Y}}) ||_p\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def log_mag_loss(\n",
    "    pred: jnp.ndarray,  # complex valued fft of the signal\n",
    "    target: jnp.ndarray,  # complex valued fft of the signal\n",
    "    eps: float = 1e-10,\n",
    "    distance: str = \"l1\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Spectral log magtinude loss but for a fft of a signal\n",
    "    See [Arik et al., 2018](https://arxiv.org/abs/1808.06719)\n",
    "    \"\"\"\n",
    "\n",
    "    pred_log_mag = log_mag(pred, eps)\n",
    "    target_log_mag = log_mag(target, eps)\n",
    "\n",
    "    # l1 spectral log magnitude loss\n",
    "    if distance == \"l1\":\n",
    "        return jnp.mean(jnp.abs(pred_log_mag - target_log_mag))\n",
    "    # l2 spectral log magnitude loss\n",
    "    elif distance == \"l2\":\n",
    "        return jnp.mean((pred_log_mag - target_log_mag) ** 2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid distance metric. Choose 'l1' or 'l2'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The spectral convergence loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{| {Y} - {\\hat{Y}} ||^2_2}{|{Y}||^2_2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def spectral_convergence_loss(\n",
    "    pred: jnp.ndarray,  # complex valued fft of the signal\n",
    "    target: jnp.ndarray,  # complex valued fft of the signal\n",
    "):\n",
    "    \"\"\"\n",
    "    Spectral convergence loss but for a fft of a signal\n",
    "    See [Arik et al., 2018](https://arxiv.org/abs/1808.06719)\n",
    "    \"\"\"\n",
    "    # l2 spectral convergence loss\n",
    "    return jnp.linalg.norm(jnp.abs(target) - jnp.abs(pred)) / jnp.linalg.norm(\n",
    "        jnp.abs(target)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from flax.training import train_state\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def mse_loss(\n",
    "    params,\n",
    "    state: train_state.TrainState,\n",
    "    x: jnp.ndarray,  # input sequence (batch, timesteps, grid_size, 1) zeros in our case\n",
    "    y: jnp.ndarray,  # output sequence (batch, timesteps, grid_size, 1) u in our case\n",
    "    dropout_key: jnp.ndarray = None,\n",
    "    norm: str = \"layer\",\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:  # loss, pred\n",
    "\n",
    "    if norm in [\"layer\"]:\n",
    "        pred = state.apply_fn({\"params\": params}, x, rngs={\"dropout\": dropout_key})\n",
    "        vars = None\n",
    "    else:\n",
    "        pred, vars = state.apply_fn(\n",
    "            {\"params\": params, \"batch_stats\": state.batch_stats},\n",
    "            x,\n",
    "            rngs={\"dropout\": dropout_key},\n",
    "            mutable=[\"batch_stats\"],\n",
    "        )\n",
    "\n",
    "    mse_loss = jnp.mean((pred - y) ** 2)\n",
    "    return mse_loss, (pred, vars)\n",
    "\n",
    "\n",
    "def fft_loss(\n",
    "    params,\n",
    "    state: train_state.TrainState,\n",
    "    x: jnp.ndarray,  # input sequence (batch, timesteps, grid_size, 1) zeros in our case\n",
    "    y: jnp.ndarray,  # output sequence (batch, timesteps, grid_size, 1) u in our case\n",
    "    dropout_key: jnp.ndarray = None,\n",
    "    norm: str = \"layer\",\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:  # loss, pred\n",
    "\n",
    "    if norm in [\"layer\"]:\n",
    "        pred = state.apply_fn(\n",
    "            {\"params\": params},\n",
    "            x,\n",
    "            rngs={\"dropout\": dropout_key},\n",
    "        )\n",
    "        vars = None\n",
    "    else:\n",
    "        pred, vars = state.apply_fn(\n",
    "            {\"params\": params, \"batch_stats\": state.batch_stats},\n",
    "            x,\n",
    "            rngs={\"dropout\": dropout_key},\n",
    "            mutable=[\"batch_stats\"],\n",
    "        )\n",
    "\n",
    "    # take the fft of the predicted and target signals\n",
    "    pred_fft = jnp.fft.rfft(pred, axis=-3)\n",
    "    y_fft = jnp.fft.rfft(y, axis=-3)\n",
    "\n",
    "    # huber_loss = jnp.mean(optax.huber_loss(pred, y, delta=0.1))\n",
    "    mse_loss = jnp.mean((pred - y) ** 2)\n",
    "\n",
    "    # magnitude mse loss\n",
    "    spec_conv_loss = jnp.mean(spectral_convergence_loss(pred_fft, y_fft))\n",
    "    # mag_mse_loss = log_mag_loss(pred_fft, y_fft, distance=\"l2\")\n",
    "    return spec_conv_loss + mse_loss, (pred, vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def lindyn_loss(\n",
    "    params,\n",
    "    state: train_state.TrainState,\n",
    "    x: jnp.ndarray,  # (B, T, G, C) pde solution\n",
    "    y: jnp.ndarray,  # (B, T, G, C) shifted pde solution\n",
    "    encdec_weight: float = 1.0,\n",
    "    lindyn_weight: float = 0.01,\n",
    "    pred_weight: float = 1.0,\n",
    "    dropout_key: jnp.ndarray = None,\n",
    "    norm: str = \"layer\",\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:  # loss, pred\n",
    "\n",
    "    params = {\"params\": params}\n",
    "\n",
    "    full_x = jnp.concatenate([x, y], axis=1)\n",
    "    # encode the initial state\n",
    "    encoded = state.apply_fn(\n",
    "        params,\n",
    "        full_x,\n",
    "        method=\"encode\",\n",
    "    )\n",
    "\n",
    "    decoded = state.apply_fn(\n",
    "        params,\n",
    "        encoded,\n",
    "        method=\"decode\",\n",
    "    )\n",
    "\n",
    "    # advance the initial state\n",
    "    # states are [1, n+1]\n",
    "    states = state.apply_fn(\n",
    "        params,\n",
    "        encoded[:, 0],\n",
    "        method=\"advance\",\n",
    "    )\n",
    "\n",
    "    # decode the encoded states\n",
    "    pred = state.apply_fn(\n",
    "        params,\n",
    "        states,\n",
    "        method=\"decode\",\n",
    "    )\n",
    "\n",
    "    # reconstruction loss between the initial state encoded and decoded\n",
    "    reconstruction_loss = jnp.mean((decoded - full_x) ** 2)\n",
    "\n",
    "    # consistency loss between the predicted encoded states and the gt encoded states\n",
    "    # compare only [1, n] with [1, n]\n",
    "    lindyn_mse_loss = jnp.mean((states - encoded[:, 1:]) ** 2)\n",
    "\n",
    "    # prediction loss\n",
    "    pred_mse_loss = jnp.mean((pred - y) ** 2)\n",
    "\n",
    "    return (\n",
    "        pred_weight * pred_mse_loss\n",
    "        + encdec_weight * reconstruction_loss\n",
    "        + lindyn_weight * lindyn_mse_loss,\n",
    "        (pred, None),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
