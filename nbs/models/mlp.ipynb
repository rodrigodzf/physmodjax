{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Sequence\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP with SELU activation and LeCun normal initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_channels: Sequence[int]  # number of hidden channels\n",
    "    activation: nn.Module = nn.selu\n",
    "    kernel_init: nn.initializers.Initializer = nn.initializers.lecun_normal()\n",
    "    use_bias: bool = True\n",
    "    layer_norm: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        for channels in self.hidden_channels:\n",
    "            x = nn.Dense(\n",
    "                features=channels,\n",
    "                kernel_init=self.kernel_init,\n",
    "                use_bias=self.use_bias,\n",
    "            )(x)\n",
    "            if channels != self.hidden_channels[-1]:\n",
    "                if self.layer_norm:\n",
    "                    x = nn.LayerNorm()(x)\n",
    "                x = self.activation(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
