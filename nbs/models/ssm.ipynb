{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S5 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from https://github.com/lindermanlab/S5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from flax import linen as nn\n",
    "from jax.nn.initializers import lecun_normal, normal\n",
    "from jax import random\n",
    "import jax.numpy as np\n",
    "from jax.nn.initializers import lecun_normal\n",
    "from jax.numpy.linalg import eigh\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def make_HiPPO(N):\n",
    "    \"\"\"Create a HiPPO-LegS matrix.\n",
    "    From https://github.com/srush/annotated-s4/blob/main/s4/s4.py\n",
    "    Args:\n",
    "        N (int32): state size\n",
    "    Returns:\n",
    "        N x N HiPPO LegS matrix\n",
    "    \"\"\"\n",
    "    P = np.sqrt(1 + 2 * np.arange(N))\n",
    "    A = P[:, np.newaxis] * P[np.newaxis, :]\n",
    "    A = np.tril(A) - np.diag(np.arange(N))\n",
    "    return -A\n",
    "\n",
    "\n",
    "def make_NPLR_HiPPO(N):\n",
    "    \"\"\"\n",
    "    Makes components needed for NPLR representation of HiPPO-LegS\n",
    "     From https://github.com/srush/annotated-s4/blob/main/s4/s4.py\n",
    "    Args:\n",
    "        N (int32): state size\n",
    "\n",
    "    Returns:\n",
    "        N x N HiPPO LegS matrix, low-rank factor P, HiPPO input matrix B\n",
    "\n",
    "    \"\"\"\n",
    "    # Make -HiPPO\n",
    "    hippo = make_HiPPO(N)\n",
    "\n",
    "    # Add in a rank 1 term. Makes it Normal.\n",
    "    P = np.sqrt(np.arange(N) + 0.5)\n",
    "\n",
    "    # HiPPO also specifies the B matrix\n",
    "    B = np.sqrt(2 * np.arange(N) + 1.0)\n",
    "    return hippo, P, B\n",
    "\n",
    "\n",
    "def make_DPLR_HiPPO(N):\n",
    "    \"\"\"\n",
    "    Makes components needed for DPLR representation of HiPPO-LegS\n",
    "     From https://github.com/srush/annotated-s4/blob/main/s4/s4.py\n",
    "    Note, we will only use the diagonal part\n",
    "    Args:\n",
    "        N:\n",
    "\n",
    "    Returns:\n",
    "        eigenvalues Lambda, low-rank term P, conjugated HiPPO input matrix B,\n",
    "        eigenvectors V, HiPPO B pre-conjugation\n",
    "\n",
    "    \"\"\"\n",
    "    A, P, B = make_NPLR_HiPPO(N)\n",
    "\n",
    "    S = A + P[:, np.newaxis] * P[np.newaxis, :]\n",
    "\n",
    "    S_diag = np.diagonal(S)\n",
    "    Lambda_real = np.mean(S_diag) * np.ones_like(S_diag)\n",
    "\n",
    "    # Diagonalize S to V \\Lambda V^*\n",
    "    Lambda_imag, V = eigh(S * -1j)\n",
    "\n",
    "    P = V.conj().T @ P\n",
    "    B_orig = B\n",
    "    B = V.conj().T @ B\n",
    "    return Lambda_real + 1j * Lambda_imag, P, B, V, B_orig\n",
    "\n",
    "\n",
    "def log_step_initializer(dt_min=0.001, dt_max=0.1):\n",
    "    \"\"\"Initialize the learnable timescale Delta by sampling\n",
    "    uniformly between dt_min and dt_max.\n",
    "    Args:\n",
    "        dt_min (float32): minimum value\n",
    "        dt_max (float32): maximum value\n",
    "    Returns:\n",
    "        init function\n",
    "    \"\"\"\n",
    "\n",
    "    def init(key, shape):\n",
    "        \"\"\"Init function\n",
    "        Args:\n",
    "            key: jax random key\n",
    "            shape tuple: desired shape\n",
    "        Returns:\n",
    "            sampled log_step (float32)\n",
    "        \"\"\"\n",
    "        return random.uniform(key, shape) * (np.log(dt_max) - np.log(dt_min)) + np.log(\n",
    "            dt_min\n",
    "        )\n",
    "\n",
    "    return init\n",
    "\n",
    "\n",
    "def init_log_steps(key, input):\n",
    "    \"\"\"Initialize an array of learnable timescale parameters\n",
    "    Args:\n",
    "        key: jax random key\n",
    "        input: tuple containing the array shape H and\n",
    "               dt_min and dt_max\n",
    "    Returns:\n",
    "        initialized array of timescales (float32): (H,)\n",
    "    \"\"\"\n",
    "    H, dt_min, dt_max = input\n",
    "    log_steps = []\n",
    "    for i in range(H):\n",
    "        key, skey = random.split(key)\n",
    "        log_step = log_step_initializer(dt_min=dt_min, dt_max=dt_max)(skey, shape=(1,))\n",
    "        log_steps.append(log_step)\n",
    "\n",
    "    return np.array(log_steps)\n",
    "\n",
    "\n",
    "def init_VinvB(init_fun, rng, shape, Vinv):\n",
    "    \"\"\"Initialize B_tilde=V^{-1}B. First samples B. Then compute V^{-1}B.\n",
    "    Note we will parameterize this with two different matrices for complex\n",
    "    numbers.\n",
    "     Args:\n",
    "         init_fun:  the initialization function to use, e.g. lecun_normal()\n",
    "         rng:       jax random key to be used with init function.\n",
    "         shape (tuple): desired shape  (P,H)\n",
    "         Vinv: (complex64)     the inverse eigenvectors used for initialization\n",
    "     Returns:\n",
    "         B_tilde (complex64) of shape (P,H,2)\n",
    "    \"\"\"\n",
    "    B = init_fun(rng, shape)\n",
    "    VinvB = Vinv @ B\n",
    "    VinvB_real = VinvB.real\n",
    "    VinvB_imag = VinvB.imag\n",
    "    return np.concatenate((VinvB_real[..., None], VinvB_imag[..., None]), axis=-1)\n",
    "\n",
    "\n",
    "def trunc_standard_normal(key, shape):\n",
    "    \"\"\"Sample C with a truncated normal distribution with standard deviation 1.\n",
    "    Args:\n",
    "        key: jax random key\n",
    "        shape (tuple): desired shape, of length 3, (H,P,_)\n",
    "    Returns:\n",
    "        sampled C matrix (float32) of shape (H,P,2) (for complex parameterization)\n",
    "    \"\"\"\n",
    "    H, P, _ = shape\n",
    "    Cs = []\n",
    "    for i in range(H):\n",
    "        key, skey = random.split(key)\n",
    "        C = lecun_normal()(skey, shape=(1, P, 2))\n",
    "        Cs.append(C)\n",
    "    return np.array(Cs)[:, 0]\n",
    "\n",
    "\n",
    "def init_CV(init_fun, rng, shape, V):\n",
    "    \"\"\"Initialize C_tilde=CV. First sample C. Then compute CV.\n",
    "    Note we will parameterize this with two different matrices for complex\n",
    "    numbers.\n",
    "     Args:\n",
    "         init_fun:  the initialization function to use, e.g. lecun_normal()\n",
    "         rng:       jax random key to be used with init function.\n",
    "         shape (tuple): desired shape  (H,P)\n",
    "         V: (complex64)     the eigenvectors used for initialization\n",
    "     Returns:\n",
    "         C_tilde (complex64) of shape (H,P,2)\n",
    "    \"\"\"\n",
    "    C_ = init_fun(rng, shape)\n",
    "    C = C_[..., 0] + 1j * C_[..., 1]\n",
    "    CV = C @ V\n",
    "    CV_real = CV.real\n",
    "    CV_imag = CV.imag\n",
    "    return np.concatenate((CV_real[..., None], CV_imag[..., None]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# Discretization functions\n",
    "def discretize_bilinear(Lambda, B_tilde, Delta):\n",
    "    \"\"\"Discretize a diagonalized, continuous-time linear SSM\n",
    "    using bilinear transform method.\n",
    "    Args:\n",
    "        Lambda (complex64): diagonal state matrix              (P,)\n",
    "        B_tilde (complex64): input matrix                      (P, H)\n",
    "        Delta (float32): discretization step sizes             (P,)\n",
    "    Returns:\n",
    "        discretized Lambda_bar (complex64), B_bar (complex64)  (P,), (P,H)\n",
    "    \"\"\"\n",
    "    Identity = np.ones(Lambda.shape[0])\n",
    "\n",
    "    BL = 1 / (Identity - (Delta / 2.0) * Lambda)\n",
    "    Lambda_bar = BL * (Identity + (Delta / 2.0) * Lambda)\n",
    "    B_bar = (BL * Delta)[..., None] * B_tilde\n",
    "    return Lambda_bar, B_bar\n",
    "\n",
    "\n",
    "def discretize_zoh(Lambda, B_tilde, Delta):\n",
    "    \"\"\"Discretize a diagonalized, continuous-time linear SSM\n",
    "    using zero-order hold method.\n",
    "    Args:\n",
    "        Lambda (complex64): diagonal state matrix              (P,)\n",
    "        B_tilde (complex64): input matrix                      (P, H)\n",
    "        Delta (float32): discretization step sizes             (P,)\n",
    "    Returns:\n",
    "        discretized Lambda_bar (complex64), B_bar (complex64)  (P,), (P,H)\n",
    "    \"\"\"\n",
    "    Identity = np.ones(Lambda.shape[0])\n",
    "    Lambda_bar = np.exp(Lambda * Delta)\n",
    "    B_bar = (1 / Lambda * (Lambda_bar - Identity))[..., None] * B_tilde\n",
    "    return Lambda_bar, B_bar\n",
    "\n",
    "\n",
    "# Parallel scan operations\n",
    "@jax.vmap\n",
    "def binary_operator(q_i, q_j):\n",
    "    \"\"\"Binary operator for parallel scan of linear recurrence. Assumes a diagonal matrix A.\n",
    "    Args:\n",
    "        q_i: tuple containing A_i and Bu_i at position i       (P,), (P,)\n",
    "        q_j: tuple containing A_j and Bu_j at position j       (P,), (P,)\n",
    "    Returns:\n",
    "        new element ( A_out, Bu_out )\n",
    "    \"\"\"\n",
    "    A_i, b_i = q_i\n",
    "    A_j, b_j = q_j\n",
    "    return A_j * A_i, A_j * b_i + b_j\n",
    "\n",
    "\n",
    "def apply_dynamics(\n",
    "    x0,\n",
    "    steps,\n",
    "    Lambda_bar,\n",
    "    B_bar,\n",
    "    C_tilde,\n",
    "    conj_sym,\n",
    "    bidirectional,\n",
    "):\n",
    "    Lambda_elements = Lambda_bar * np.ones((steps, Lambda_bar.shape[0]))\n",
    "    h0 = B_bar @ x0\n",
    "    xs = jax.lax.associative_scan(np.multiply, Lambda_elements) * h0\n",
    "\n",
    "    if bidirectional:\n",
    "        xs2 = jax.lax.associative_scan(np.multiply, Lambda_elements, reverse=True) * h0\n",
    "        xs = np.concatenate((xs, xs2), axis=-1)\n",
    "\n",
    "    if conj_sym:\n",
    "        return jax.vmap(lambda x: 2 * (C_tilde @ x).real)(xs)\n",
    "    else:\n",
    "        return jax.vmap(lambda x: (C_tilde @ x).real)(xs)\n",
    "\n",
    "\n",
    "def apply_ssm(\n",
    "    Lambda_bar,\n",
    "    B_bar,\n",
    "    C_tilde,\n",
    "    input_sequence,\n",
    "    conj_sym,\n",
    "    bidirectional,\n",
    "):\n",
    "    \"\"\"Compute the LxH output of discretized SSM given an LxH input.\n",
    "    Args:\n",
    "        Lambda_bar (complex64): discretized diagonal state matrix    (P,)\n",
    "        B_bar      (complex64): discretized input matrix             (P, H)\n",
    "        C_tilde    (complex64): output matrix                        (H, P)\n",
    "        input_sequence (float32): input sequence of features         (L, H)\n",
    "        conj_sym (bool):         whether conjugate symmetry is enforced\n",
    "        bidirectional (bool):    whether bidirectional setup is used,\n",
    "                              Note for this case C_tilde will have 2P cols\n",
    "    Returns:\n",
    "        ys (float32): the SSM outputs (S5 layer preactivations)      (L, H)\n",
    "    \"\"\"\n",
    "    Lambda_elements = Lambda_bar * np.ones(\n",
    "        (input_sequence.shape[0], Lambda_bar.shape[0])\n",
    "    )\n",
    "\n",
    "    Bu_elements = jax.vmap(lambda u: B_bar @ u)(input_sequence)\n",
    "\n",
    "    _, xs = jax.lax.associative_scan(binary_operator, (Lambda_elements, Bu_elements))\n",
    "\n",
    "    if bidirectional:\n",
    "        _, xs2 = jax.lax.associative_scan(\n",
    "            binary_operator, (Lambda_elements, Bu_elements), reverse=True\n",
    "        )\n",
    "        xs = np.concatenate((xs, xs2), axis=-1)\n",
    "\n",
    "    if conj_sym:\n",
    "        return jax.vmap(lambda x: 2 * (C_tilde @ x).real)(xs)\n",
    "    else:\n",
    "        return jax.vmap(lambda x: (C_tilde @ x).real)(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class S5SSM(nn.Module):\n",
    "    d_model: int\n",
    "    d_hidden: int\n",
    "    C_init: str = \"lecun_normal\"\n",
    "    discretization: str = \"zoh\"\n",
    "    dt_min: float = 0.0001\n",
    "    dt_max: float = 0.1\n",
    "    conj_sym: bool = True\n",
    "    clip_eigs: bool = False\n",
    "    bidirectional: bool = False\n",
    "    step_rescale: float = 1.0\n",
    "    blocks: int = 16\n",
    "    n_steps: Optional[int] = None\n",
    "\n",
    "    \"\"\" The S5 SSM\n",
    "        Args:\n",
    "            Lambda_re_init (complex64): Real part of init diag state matrix  (P,)\n",
    "            Lambda_im_init (complex64): Imag part of init diag state matrix  (P,)\n",
    "            V           (complex64): Eigenvectors used for init           (P,P)\n",
    "            Vinv        (complex64): Inverse eigenvectors used for init   (P,P)\n",
    "            d_model     (int32):     Number of features of input seq \n",
    "            d_hidden    (int32):     state size\n",
    "            C_init      (string):    Specifies How C is initialized\n",
    "                         Options: [trunc_standard_normal: sample from truncated standard normal \n",
    "                                                        and then multiply by V, i.e. C_tilde=CV.\n",
    "                                   lecun_normal: sample from Lecun_normal and then multiply by V.\n",
    "                                   complex_normal: directly sample a complex valued output matrix \n",
    "                                                    from standard normal, does not multiply by V]\n",
    "            conj_sym    (bool):    Whether conjugate symmetry is enforced\n",
    "            clip_eigs   (bool):    Whether to enforce left-half plane condition, i.e.\n",
    "                                   constrain real part of eigenvalues to be negative. \n",
    "                                   True recommended for autoregressive task/unbounded sequence lengths\n",
    "                                   Discussed in https://arxiv.org/pdf/2206.11893.pdf.\n",
    "            bidirectional (bool):  Whether model is bidirectional, if True, uses two C matrices\n",
    "            discretization: (string) Specifies discretization method \n",
    "                             options: [zoh: zero-order hold method,\n",
    "                                       bilinear: bilinear transform]\n",
    "            dt_min:      (float32): minimum value to draw timescale values from when \n",
    "                                    initializing log_step\n",
    "            dt_max:      (float32): maximum value to draw timescale values from when \n",
    "                                    initializing log_step\n",
    "            step_rescale:  (float32): allows for uniformly changing the timescale parameter, e.g. after training \n",
    "                                    on a different resolution for the speech commands benchmark\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Initializes parameters once and performs discretization each time\n",
    "        the SSM is applied to a sequence\n",
    "        \"\"\"\n",
    "        self.H = self.d_model\n",
    "        self.P = self.d_hidden\n",
    "\n",
    "        # Initialize state matrix A using approximation to HiPPO-LegS matrix\n",
    "\n",
    "        block_size = int(self.P / self.blocks)\n",
    "        # Initialize state matrix A using approximation to HiPPO-LegS matrix\n",
    "        Lambda, _, B, V, B_orig = make_DPLR_HiPPO(block_size)\n",
    "\n",
    "        if self.conj_sym:\n",
    "            # Need to account for case where we actually sample real B and C, and then multiply\n",
    "            # by the half sized Vinv and possibly V\n",
    "            block_size = block_size // 2\n",
    "            P = self.P // 2\n",
    "            local_P = 2 * P\n",
    "        else:\n",
    "            local_P = P\n",
    "\n",
    "        Lambda = Lambda[:block_size]\n",
    "        V = V[:, :block_size]\n",
    "        Vc = V.conj().T\n",
    "\n",
    "        # If initializing state matrix A as block-diagonal, put HiPPO approximation\n",
    "        # on each block\n",
    "        Lambda = (Lambda * np.ones((self.blocks, block_size))).ravel()\n",
    "        self.V = jax.scipy.linalg.block_diag(*([V] * self.blocks))\n",
    "        self.Vinv = jax.scipy.linalg.block_diag(*([Vc] * self.blocks))\n",
    "\n",
    "        # Initialize diagonal state to state matrix Lambda (eigenvalues)\n",
    "        self.Lambda_re = self.param(\n",
    "            \"Lambda_re\", lambda rng, shape: Lambda.real, (None,)\n",
    "        )\n",
    "        self.Lambda_im = self.param(\n",
    "            \"Lambda_im\", lambda rng, shape: Lambda.imag, (None,)\n",
    "        )\n",
    "        if self.clip_eigs:\n",
    "            self.Lambda = np.clip(self.Lambda_re, None, -1e-4) + 1j * self.Lambda_im\n",
    "        else:\n",
    "            self.Lambda = self.Lambda_re + 1j * self.Lambda_im\n",
    "\n",
    "        # Initialize input to state (B) matrix\n",
    "        B_init = lecun_normal()\n",
    "        B_shape = (local_P, self.H)\n",
    "        self.B = self.param(\n",
    "            \"B\", lambda rng, shape: init_VinvB(B_init, rng, shape, self.Vinv), B_shape\n",
    "        )\n",
    "        B_tilde = self.B[..., 0] + 1j * self.B[..., 1]\n",
    "\n",
    "        # Initialize state to output (C) matrix\n",
    "        if self.C_init in [\"trunc_standard_normal\"]:\n",
    "            C_init = trunc_standard_normal\n",
    "            C_shape = (self.H, local_P, 2)\n",
    "        elif self.C_init in [\"lecun_normal\"]:\n",
    "            C_init = lecun_normal()\n",
    "            C_shape = (self.H, local_P, 2)\n",
    "        elif self.C_init in [\"complex_normal\"]:\n",
    "            C_init = normal(stddev=0.5**0.5)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"C_init method {} not implemented\".format(self.C_init)\n",
    "            )\n",
    "\n",
    "        if self.C_init in [\"complex_normal\"]:\n",
    "            if self.bidirectional:\n",
    "                C = self.param(\"C\", C_init, (self.H, 2 * P, 2))\n",
    "                self.C_tilde = C[..., 0] + 1j * C[..., 1]\n",
    "\n",
    "            else:\n",
    "                C = self.param(\"C\", C_init, (self.H, P, 2))\n",
    "                self.C_tilde = C[..., 0] + 1j * C[..., 1]\n",
    "\n",
    "        else:\n",
    "            if self.bidirectional:\n",
    "                self.C1 = self.param(\n",
    "                    \"C1\",\n",
    "                    lambda rng, shape: init_CV(C_init, rng, shape, self.V),\n",
    "                    C_shape,\n",
    "                )\n",
    "                self.C2 = self.param(\n",
    "                    \"C2\",\n",
    "                    lambda rng, shape: init_CV(C_init, rng, shape, self.V),\n",
    "                    C_shape,\n",
    "                )\n",
    "\n",
    "                C1 = self.C1[..., 0] + 1j * self.C1[..., 1]\n",
    "                C2 = self.C2[..., 0] + 1j * self.C2[..., 1]\n",
    "                self.C_tilde = np.concatenate((C1, C2), axis=-1)\n",
    "\n",
    "            else:\n",
    "                self.C = self.param(\n",
    "                    \"C\", lambda rng, shape: init_CV(C_init, rng, shape, self.V), C_shape\n",
    "                )\n",
    "\n",
    "                self.C_tilde = self.C[..., 0] + 1j * self.C[..., 1]\n",
    "\n",
    "        # Initialize feedthrough (D) matrix\n",
    "        self.D = self.param(\"D\", normal(stddev=1.0), (self.H,))\n",
    "\n",
    "        # Initialize learnable discretization timescale value\n",
    "        self.log_step = self.param(\n",
    "            \"log_step\", init_log_steps, (P, self.dt_min, self.dt_max)\n",
    "        )\n",
    "        step = self.step_rescale * np.exp(self.log_step[:, 0])\n",
    "\n",
    "        # Discretize\n",
    "        if self.discretization in [\"zoh\"]:\n",
    "            self.Lambda_bar, self.B_bar = discretize_zoh(self.Lambda, B_tilde, step)\n",
    "        elif self.discretization in [\"bilinear\"]:\n",
    "            self.Lambda_bar, self.B_bar = discretize_bilinear(\n",
    "                self.Lambda, B_tilde, step\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Discretization method {} not implemented\".format(self.discretization)\n",
    "            )\n",
    "\n",
    "    def __call__(self, input_sequence):\n",
    "        \"\"\"\n",
    "        Compute the LxH output of the S5 SSM given an LxH input sequence\n",
    "        using a parallel scan.\n",
    "        Args:\n",
    "             input_sequence (float32): input sequence (L, H)\n",
    "        Returns:\n",
    "            output sequence (float32): (L, H)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.n_steps:\n",
    "            ys = apply_dynamics(\n",
    "                input_sequence[0],\n",
    "                self.n_steps,\n",
    "                self.Lambda_bar,\n",
    "                self.B_bar,\n",
    "                self.C_tilde,\n",
    "                self.conj_sym,\n",
    "                self.bidirectional,\n",
    "            )\n",
    "            return ys\n",
    "        else:\n",
    "            ys = apply_ssm(\n",
    "                self.Lambda_bar,\n",
    "                self.B_bar,\n",
    "                self.C_tilde,\n",
    "                input_sequence,\n",
    "                self.conj_sym,\n",
    "                self.bidirectional,\n",
    "            )\n",
    "            # Add feedthrough matrix output Du;\n",
    "            Du = jax.vmap(lambda u: self.D * u)(input_sequence)\n",
    "            return ys + Du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from https://github.com/NicolasZucchet/minimal-LRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def matrix_init(key, shape, dtype=jnp.float32, normalization=1):\n",
    "    return jax.random.normal(key=key, shape=shape, dtype=dtype) / normalization\n",
    "\n",
    "\n",
    "def nu_init(key, shape, r_min, r_max, dtype=jnp.float32):\n",
    "    u = jax.random.uniform(key=key, shape=shape, dtype=dtype)\n",
    "    return jnp.log(-0.5 * jnp.log(u * (r_max**2 - r_min**2) + r_min**2))\n",
    "\n",
    "\n",
    "def theta_init(key, shape, max_phase, dtype=jnp.float32):\n",
    "    u = jax.random.uniform(key, shape=shape, dtype=dtype)\n",
    "    return jnp.log(max_phase * u)\n",
    "\n",
    "\n",
    "def gamma_log_init(key, lamb):\n",
    "    nu, theta = lamb\n",
    "    diag_lambda = jnp.exp(-jnp.exp(nu) + 1j * jnp.exp(theta))\n",
    "    return jnp.log(jnp.sqrt(1 - jnp.abs(diag_lambda) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class LRUDynamics(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements only the dynamics of the LRU model.\n",
    "    x_{k+1} = A x_k\n",
    "    \"\"\"\n",
    "\n",
    "    d_hidden: int  # hidden state dimension\n",
    "    r_min: float  # smallest eigenvalue radius\n",
    "    r_max: float  # largest eigenvalue radius\n",
    "    max_phase: float  # largest phase\n",
    "    clip_eigs: bool  # whether to clip the eigenvalues\n",
    "\n",
    "    def setup(self):\n",
    "        self.theta_log = self.param(\n",
    "            \"theta_log\", partial(theta_init, max_phase=self.max_phase), (self.d_hidden,)\n",
    "        )\n",
    "        self.nu_log = self.param(\n",
    "            \"nu_log\",\n",
    "            partial(nu_init, r_min=self.r_min, r_max=self.r_max),\n",
    "            (self.d_hidden,),\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,  # initial complex state flattened (d_hidden,) complex\n",
    "        steps: int,  # number of steps to advance\n",
    "    ) -> jnp.ndarray:  # advanced state (steps, d_hidden) complex\n",
    "\n",
    "        A_real = -jnp.exp(self.nu_log)\n",
    "        A_imag = jnp.exp(self.theta_log)\n",
    "\n",
    "        # clip the eigenvalues to be only negative (not strictly necessary, because of the extra log)\n",
    "        if self.clip_eigs:\n",
    "            A_real = jnp.clip(A_real, None, -1e-5)\n",
    "\n",
    "        A_diag = jnp.exp(A_real + 1j * A_imag)\n",
    "        A_seq = jnp.repeat(A_diag[None, :], steps, axis=0)\n",
    "\n",
    "        # advance the state\n",
    "        x = jax.lax.associative_scan(jnp.multiply, A_seq) * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "d_hidden = 64\n",
    "steps = 50\n",
    "dyn = LRUDynamics(d_hidden=d_hidden, r_min=0.99, r_max=1.0, max_phase=jnp.pi * 2, clip_eigs=False)\n",
    "vars = dyn.init(jax.random.PRNGKey(0), jnp.ones((d_hidden)), 50)\n",
    "out = dyn.apply(vars, jnp.ones((1, d_hidden)), 50)\n",
    "\n",
    "assert out.shape == (steps, d_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def apply_lru_dynamics(\n",
    "    inputs: jnp.ndarray,  # (time, d_model)\n",
    "    discrete_lambda: jnp.ndarray,  # (d_hidden,)\n",
    "    B_norm: jnp.ndarray,  # (d_hidden, d_model)\n",
    "    C: jnp.ndarray,  # (d_model, d_hidden)\n",
    "    D: jnp.ndarray,  # (d_model,)\n",
    "):\n",
    "\n",
    "    Lambda_elements = jnp.repeat(discrete_lambda[None, ...], inputs.shape[0], axis=0)\n",
    "\n",
    "    Bu_elements = jax.vmap(lambda u: B_norm @ u)(inputs)\n",
    "    _, hidden_states = jax.lax.associative_scan(\n",
    "        binary_operator, (Lambda_elements, Bu_elements)\n",
    "    )\n",
    "    return jax.vmap(lambda h, x: (C @ h).real + D * x)(hidden_states, inputs)\n",
    "\n",
    "\n",
    "def apply_lru_dynamics_from_ic(\n",
    "    ic: jnp.ndarray,  # (1, d_model)\n",
    "    n_steps: int,\n",
    "    discrete_lambda: jnp.ndarray,  # (d_hidden,)\n",
    "    B_norm: jnp.ndarray,  # (d_hidden, d_model)\n",
    "    C: jnp.ndarray,  # (d_model, d_hidden)\n",
    "):\n",
    "\n",
    "    Lambda_elements = jnp.repeat(discrete_lambda[None, ...], n_steps, axis=0)\n",
    "    h0 = B_norm @ ic[0]\n",
    "    hidden_states = jax.lax.associative_scan(jnp.multiply, Lambda_elements) * h0\n",
    "    return jax.vmap(lambda h: (C @ h).real)(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    \"\"\"\n",
    "    LRU module in charge of the recurrent processing.\n",
    "    Implementation following the one of Orvieto et al. 2023.\n",
    "    \"\"\"\n",
    "\n",
    "    d_hidden: int  # hidden state dimension\n",
    "    d_model: int  # input and output dimensions\n",
    "    r_min: float = 0.0  # smallest lambda norm\n",
    "    r_max: float = 1.0  # largest lambda norm\n",
    "    max_phase: float = 6.28  # max phase lambda\n",
    "    n_steps: Optional[int] = None  # number of steps to advance\n",
    "\n",
    "    def setup(self):\n",
    "        theta_log = self.param(\n",
    "            \"theta_log\", partial(theta_init, max_phase=self.max_phase), (self.d_hidden,)\n",
    "        )\n",
    "        nu_log = self.param(\n",
    "            \"nu_log\",\n",
    "            partial(nu_init, r_min=self.r_min, r_max=self.r_max),\n",
    "            (self.d_hidden,),\n",
    "        )\n",
    "        gamma_log = self.param(\"gamma_log\", gamma_log_init, (nu_log, theta_log))\n",
    "\n",
    "        # Glorot initialized Input/Output projection matrices\n",
    "        B_re = self.param(\n",
    "            \"B_re\",\n",
    "            partial(matrix_init, normalization=jnp.sqrt(2 * self.d_model)),\n",
    "            (self.d_hidden, self.d_model),\n",
    "        )\n",
    "        B_im = self.param(\n",
    "            \"B_im\",\n",
    "            partial(matrix_init, normalization=jnp.sqrt(2 * self.d_model)),\n",
    "            (self.d_hidden, self.d_model),\n",
    "        )\n",
    "        C_re = self.param(\n",
    "            \"C_re\",\n",
    "            partial(matrix_init, normalization=jnp.sqrt(self.d_hidden)),\n",
    "            (self.d_model, self.d_hidden),\n",
    "        )\n",
    "        C_im = self.param(\n",
    "            \"C_im\",\n",
    "            partial(matrix_init, normalization=jnp.sqrt(self.d_hidden)),\n",
    "            (self.d_model, self.d_hidden),\n",
    "        )\n",
    "        self.D = self.param(\"D\", matrix_init, (self.d_model,))\n",
    "\n",
    "        self.C = C_re + 1j * C_im\n",
    "        B = B_re + 1j * B_im\n",
    "        self.B_norm = B * jnp.exp(gamma_log)[..., None]\n",
    "\n",
    "        self.discrete_diag_lambda = jnp.exp(-jnp.exp(nu_log) + 1j * jnp.exp(theta_log))\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        inputs: jnp.ndarray,  # (time, d_model)\n",
    "    ):\n",
    "        if self.n_steps is not None:\n",
    "            return apply_lru_dynamics_from_ic(\n",
    "                inputs,\n",
    "                self.n_steps,\n",
    "                self.discrete_diag_lambda,\n",
    "                self.B_norm,\n",
    "                self.C,\n",
    "            )\n",
    "        else:\n",
    "            return apply_lru_dynamics(\n",
    "                inputs,\n",
    "                self.discrete_diag_lambda,\n",
    "                self.B_norm,\n",
    "                self.C,\n",
    "                self.D,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep (Stacked) and Batched versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class SequenceLayer(nn.Module):\n",
    "    \"\"\"Single layer, with one SSM module, GLU, dropout and batch/layer norm\"\"\"\n",
    "\n",
    "    ssm: nn.Module  # ssm module\n",
    "    d_model: int  # model size\n",
    "    dropout: float = 0.0  # dropout probability\n",
    "    norm: str = \"layer\"  # which normalization to use\n",
    "    training: bool = True  # in training mode (dropout in trainign mode only)\n",
    "    activation: str = \"half_glu1\"  # activation function\n",
    "    prenorm: bool = True  # whether to use pre or post normalization\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Initializes the ssm, layer norm and dropout\"\"\"\n",
    "        self.seq = self.ssm()\n",
    "        self.out1 = nn.Dense(self.d_model)\n",
    "        self.out2 = nn.Dense(self.d_model)\n",
    "        if self.norm in [\"layer\"]:\n",
    "            self.normalization = nn.LayerNorm()\n",
    "        else:\n",
    "            self.normalization = nn.BatchNorm(\n",
    "                use_running_average=not self.training, axis_name=\"batch\"\n",
    "            )\n",
    "        self.drop = nn.Dropout(\n",
    "            self.dropout, broadcast_dims=[0], deterministic=not self.training\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        skip = x\n",
    "        if self.prenorm:\n",
    "            x = self.normalization(x)  # pre normalization\n",
    "        x = self.seq(x)  # call LRU\n",
    "        if self.activation in [\"full_glu\"]:\n",
    "            x = self.drop(nn.gelu(x))\n",
    "            x = self.out1(x) * jax.nn.sigmoid(self.out2(x))\n",
    "            x = self.drop(x)\n",
    "        elif self.activation in [\"half_glu1\"]:\n",
    "            x = self.drop(nn.gelu(x))\n",
    "            x = x * jax.nn.sigmoid(self.out2(x))\n",
    "            x = self.drop(x)\n",
    "        elif self.activation in [\"gelu\"]:\n",
    "            x = self.drop(nn.gelu(x))\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Activation {self.activation} not implemented\")\n",
    "        x = skip + x  # skip connection\n",
    "        if not self.prenorm:\n",
    "            x = self.normalization(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class StackedSSM(nn.Module):\n",
    "\n",
    "    ssm: nn.Module  # ssm module\n",
    "    d_model: int  # model size\n",
    "    d_vars: int  # number of variables\n",
    "    n_layers: int  # number of layers\n",
    "    ssm_first_layer: nn.Module = None  # optional first layer usually for one-to-many\n",
    "    n_steps: Optional[int] = None  # number of steps to advance\n",
    "    dropout: float = 0.0  # dropout probability\n",
    "    training: bool = True\n",
    "    norm: str = \"layer\"\n",
    "    activation: str = \"half_glu1\"\n",
    "    prenorm: bool = True\n",
    "\n",
    "    def setup(self):\n",
    "        if self.ssm_first_layer is not None:\n",
    "            self.first_layer = self.ssm_first_layer(\n",
    "                d_model=self.d_model * self.d_vars,\n",
    "                n_steps=self.n_steps,\n",
    "            )\n",
    "        self.layers = [\n",
    "            SequenceLayer(\n",
    "                ssm=partial(self.ssm, d_model=self.d_model * self.d_vars),\n",
    "                d_model=self.d_model * self.d_vars,\n",
    "                dropout=self.dropout,\n",
    "                training=self.training,\n",
    "                norm=self.norm,\n",
    "                activation=self.activation,\n",
    "                prenorm=self.prenorm,\n",
    "            )\n",
    "            for _ in range(self.n_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,  # (T, ...) or (W, C) # input\n",
    "    ):\n",
    "        x = rearrange(x, \"t w c -> t (w c)\")\n",
    "\n",
    "        if self.ssm_first_layer is not None:\n",
    "            x = self.first_layer(x)\n",
    "        else:\n",
    "            x = jnp.concatenate(\n",
    "                [x[0:1], jnp.zeros((x.shape[0] - 1, x.shape[1]))], axis=0\n",
    "            )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # apply each layer\n",
    "\n",
    "        return rearrange(x, \"t (w c) -> t w c\", w=self.d_model, c=self.d_vars)\n",
    "\n",
    "\n",
    "BatchStackedSSMModel = nn.vmap(\n",
    "    StackedSSM,\n",
    "    in_axes=0,\n",
    "    out_axes=0,\n",
    "    variable_axes={\n",
    "        \"params\": None,\n",
    "        \"dropout\": None,\n",
    "        \"batch_stats\": None,\n",
    "        \"cache\": 0,\n",
    "        \"prime\": None,\n",
    "    },\n",
    "    split_rngs={\"params\": False, \"dropout\": True},\n",
    "    axis_name=\"batch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "B, T, W, C = 10, 50, 20, 3\n",
    "d_hidden = 64\n",
    "deep_ssm = BatchStackedSSMModel(\n",
    "    ssm_first_layer=partial(S5SSM, d_hidden=d_hidden, n_steps=50),\n",
    "    ssm=partial(S5SSM, d_hidden=d_hidden),\n",
    "    d_model=W,\n",
    "    d_vars=C,\n",
    "    n_layers=2,\n",
    ")\n",
    "x = jnp.empty((B, T, W, C))\n",
    "variables = deep_ssm.init(jax.random.PRNGKey(65), x)\n",
    "out = deep_ssm.apply(variables, x)\n",
    "\n",
    "assert out.shape == (B, T, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "deep_ssm = BatchStackedSSMModel(\n",
    "    ssm_first_layer=partial(LRU, d_hidden=d_hidden, n_steps=50),\n",
    "    ssm=partial(LRU, d_hidden=d_hidden),\n",
    "    d_model=W,\n",
    "    d_vars=C,\n",
    "    n_layers=2,\n",
    ")\n",
    "x = jnp.empty((B, T, W, C))\n",
    "variables = deep_ssm.init(jax.random.PRNGKey(65), x)\n",
    "out = deep_ssm.apply(variables, x)\n",
    "\n",
    "assert out.shape == (B, T, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class StackedSSM2D(nn.Module):\n",
    "\n",
    "    ssm: nn.Module  # ssm module\n",
    "    d_model: Tuple[int, int]\n",
    "    d_vars: int  # number of variables\n",
    "    n_layers: int  # number of layers\n",
    "    ssm_first_layer: nn.Module = None  # optional first layer usually for one-to-many\n",
    "    n_steps: Optional[int] = None  # number of steps to advance\n",
    "    dropout: float = 0.0  # dropout probability\n",
    "    training: bool = True\n",
    "    norm: str = \"layer\"\n",
    "    activation: str = \"half_glu1\"\n",
    "    prenorm: bool = True\n",
    "\n",
    "    def setup(self):\n",
    "        if self.ssm_first_layer is not None:\n",
    "            self.first_layer = self.ssm_first_layer(\n",
    "                d_model=self.d_model[0] * self.d_model[1] * self.d_vars,\n",
    "                n_steps=self.n_steps,\n",
    "            )\n",
    "        self.layers = [\n",
    "            SequenceLayer(\n",
    "                ssm=partial(\n",
    "                    self.ssm, d_model=self.d_model[0] * self.d_model[1] * self.d_vars\n",
    "                ),\n",
    "                d_model=self.d_model[0] * self.d_model[1] * self.d_vars,\n",
    "                dropout=self.dropout,\n",
    "                training=self.training,\n",
    "                norm=self.norm,\n",
    "                activation=self.activation,\n",
    "                prenorm=self.prenorm,\n",
    "            )\n",
    "            for _ in range(self.n_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,  # (T, H, W, C) or (H, W, C) # input\n",
    "    ):\n",
    "\n",
    "        x = rearrange(x, \"t h w c -> t (h w c)\")\n",
    "\n",
    "        if self.ssm_first_layer is not None:\n",
    "            x = self.first_layer(x)\n",
    "        else:\n",
    "            x = jnp.concatenate(\n",
    "                [x[0:1], jnp.zeros((x.shape[0] - 1, x.shape[1]))], axis=0\n",
    "            )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # apply each layer\n",
    "\n",
    "        return rearrange(\n",
    "            x,\n",
    "            \"t (h w c) -> t h w c\",\n",
    "            h=self.d_model[0],\n",
    "            w=self.d_model[1],\n",
    "            c=self.d_vars,\n",
    "        )\n",
    "\n",
    "\n",
    "BatchStackedSSM2DModel = nn.vmap(\n",
    "    StackedSSM2D,\n",
    "    in_axes=0,\n",
    "    out_axes=0,\n",
    "    variable_axes={\n",
    "        \"params\": None,\n",
    "        \"dropout\": None,\n",
    "        \"batch_stats\": None,\n",
    "        \"cache\": 0,\n",
    "        \"prime\": None,\n",
    "    },\n",
    "    split_rngs={\"params\": False, \"dropout\": True},\n",
    "    axis_name=\"batch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "B, T, H, W, C = 10, 50, 20, 20, 3\n",
    "deep_ssm = BatchStackedSSM2DModel(\n",
    "    ssm_first_layer=partial(LRU, d_hidden=d_hidden, n_steps=T),\n",
    "    ssm=partial(LRU, d_hidden=d_hidden),\n",
    "    d_model=(H, W),\n",
    "    d_vars=C,\n",
    "    n_layers=2,\n",
    ")\n",
    "\n",
    "x = jnp.empty((B, T, H, W, C))\n",
    "variables = deep_ssm.init(jax.random.PRNGKey(65), x)\n",
    "out = deep_ssm.apply(variables, x)\n",
    "\n",
    "assert out.shape == (B, T, H, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "B, T, H, W, C = 10, 50, 20, 20, 3\n",
    "deep_ssm = BatchStackedSSM2DModel(\n",
    "    ssm_first_layer=partial(S5SSM, d_hidden=d_hidden, n_steps=T),\n",
    "    ssm=partial(S5SSM, d_hidden=d_hidden),\n",
    "    d_model=(H, W),\n",
    "    d_vars=C,\n",
    "    n_layers=2,\n",
    ")\n",
    "\n",
    "x = jnp.empty((B, T, H, W, C))\n",
    "variables = deep_ssm.init(jax.random.PRNGKey(65), x)\n",
    "out = deep_ssm.apply(variables, x)\n",
    "\n",
    "assert out.shape == (B, T, H, W, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
