{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from functools import partial\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRU dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear dynamics using initisialisation of the eigenvalues based on the LRU paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from physmodjax.models.ssm import (\n",
    "    theta_init,\n",
    "    nu_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from physmodjax.models.ssm import LRUDynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRU with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class LRUDynamicsVarying(LRUDynamics):\n",
    "\n",
    "    model: nn.Module  # model to process the linear state\n",
    "\n",
    "    def setup(self):\n",
    "        super().setup()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,  # initial complex state flattened (d_hidden,) complex\n",
    "        steps: int,  # number of steps to advance\n",
    "    ) -> jnp.ndarray:  # advanced state (steps, d_hidden) complex\n",
    "\n",
    "        x = super().__call__(x, steps)\n",
    "        x_hat = self.model(x.real**2 + x.imag**2)\n",
    "        x_hat = x_hat[..., : self.d_hidden] + 1j * x_hat[..., self.d_hidden :]\n",
    "        x = x * x_hat\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physmodjax.models.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "d_hidden = 64\n",
    "steps = 50\n",
    "model = MLP(hidden_channels=[64, 64, 64])\n",
    "dyn = LRUDynamicsVarying(\n",
    "    d_hidden=d_hidden,\n",
    "    r_min=0.99,\n",
    "    r_max=1.0,\n",
    "    max_phase=jnp.pi * 2,\n",
    "    model=model,\n",
    "    clip_eigs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DeepRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep RNN model that applies a RNN cell over the last dimension of the input.\n",
    "    Works with nn.GRUCell, nn.RNNCell, nn.SimpleCell, nn.MGUCell.\n",
    "    \"\"\"\n",
    "\n",
    "    d_model: int\n",
    "    d_vars: int\n",
    "    n_layers: int\n",
    "    cell: nn.Module\n",
    "    training: bool = True\n",
    "    norm: str = \"layer\"\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        # scan does the same thing as nn.RNN (unrolls the over the time dimension)\n",
    "        self.first_layer = nn.RNN(\n",
    "            self.cell(features=self.d_model * self.d_vars),\n",
    "        )\n",
    "\n",
    "        self.layers = [\n",
    "            nn.RNN(\n",
    "                self.cell(features=self.d_model * self.d_vars),\n",
    "            )\n",
    "            for _ in range(self.n_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x0: jnp.ndarray,  # (W, C) # initial state\n",
    "        x: jnp.ndarray,  # (T, W, C) # empty state\n",
    "    ) -> jnp.ndarray:  # (T, W, C) # advanced state\n",
    "        # the rnn works over the last dimension\n",
    "        # we need to reshape the input to (T, d_model * C)\n",
    "        x0 = rearrange(x0, \"w c -> (w c)\")\n",
    "        x = rearrange(x, \"t w c -> t (w c)\")\n",
    "        x = self.first_layer(x, initial_carry=x0)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return rearrange(x, \"t (w c) -> t w c\", w=self.d_model, c=self.d_vars)\n",
    "\n",
    "\n",
    "BatchedDeepRNN = nn.vmap(\n",
    "    DeepRNN,\n",
    "    in_axes=0,\n",
    "    out_axes=0,\n",
    "    variable_axes={\"params\": None},\n",
    "    split_rngs={\"params\": False},\n",
    "    axis_name=\"batch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "B, T, W, C = 10, 50, 20, 3\n",
    "deep_rnn = BatchedDeepRNN(d_model=W, d_vars=C, n_layers=2, cell=partial(nn.GRUCell))\n",
    "x = jnp.ones((B, T, W, C))\n",
    "x0 = jnp.ones((B, W, C))\n",
    "variables = deep_rnn.init(jax.random.PRNGKey(65), x0, x)\n",
    "out = deep_rnn.apply(variables, x0, x)\n",
    "\n",
    "assert out.shape == (B, T, W, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
