{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from typing import Tuple\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "Conv3x3 = partial(nn.Conv, kernel_size=(3, 3))\n",
    "\n",
    "\n",
    "class ConvRelu2(nn.Module):\n",
    "    \"\"\"Two unpadded convolutions & relus.\n",
    "\n",
    "    Attributes:\n",
    "      features: Num convolutional features.\n",
    "      padding: Type of padding: 'SAME' or 'VALID'.\n",
    "      norm: Whether to use batchnorm at the end or not.\n",
    "    \"\"\"\n",
    "\n",
    "    features: int\n",
    "    padding: str = \"SAME\"\n",
    "    norm: str = \"layer\"\n",
    "    training: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        x = Conv3x3(features=self.features, name=\"conv1\", padding=self.padding)(x)\n",
    "        if self.norm in [\"batch\"]:\n",
    "            x = nn.BatchNorm(use_running_average=not self.training, axis_name=\"batch\")(\n",
    "                x\n",
    "            )\n",
    "        x = nn.relu(x)\n",
    "        x = Conv3x3(features=self.features, name=\"conv2\", padding=self.padding)(x)\n",
    "        if self.norm in [\"batch\"]:\n",
    "            x = nn.BatchNorm(use_running_average=not self.training, axis_name=\"batch\")(\n",
    "                x\n",
    "            )\n",
    "        x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeConv3x3(nn.Module):\n",
    "    \"\"\"Deconvolution layer for upscaling.\n",
    "\n",
    "    Attributes:\n",
    "      features: Num convolutional features.\n",
    "      padding: Type of padding: 'SAME' or 'VALID'.\n",
    "      norm: Whether to use batchnorm at the end or not.\n",
    "    \"\"\"\n",
    "\n",
    "    features: int\n",
    "    padding: str = \"SAME\"\n",
    "    norm: str = \"layer\"\n",
    "    training: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Applies transposed convolution with 3x3 kernel.\"\"\"\n",
    "        # NOTE: In the scenic code this is a deconvolution.\n",
    "\n",
    "        if self.padding == \"SAME\":\n",
    "            padding = ((1, 2), (1, 2))\n",
    "        elif self.padding == \"VALID\":\n",
    "            padding = ((0, 0), (0, 0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unkonwn padding: {self.padding}\")\n",
    "        x = nn.Conv(\n",
    "            features=self.features,\n",
    "            kernel_size=(3, 3),\n",
    "            input_dilation=(2, 2),\n",
    "            padding=padding,\n",
    "        )(x)\n",
    "        if self.norm in [\"batch\"]:\n",
    "            x = nn.BatchNorm(use_running_average=not self.training, axis_name=\"batch\")(\n",
    "                x\n",
    "            )\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    \"\"\"Two unpadded convolutions & downsample 2x.\n",
    "\n",
    "    Attributes:\n",
    "      features: Num convolutional features.\n",
    "      padding: Type of padding: 'SAME' or 'VALID'.\n",
    "      norm: Whether to use batchnorm at the end or not.\n",
    "    \"\"\"\n",
    "\n",
    "    features: int\n",
    "    padding: str = \"SAME\"\n",
    "    norm: str = \"layer\"\n",
    "    training: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        x = ConvRelu2(\n",
    "            features=self.features,\n",
    "            padding=self.padding,\n",
    "            norm=self.norm,\n",
    "            training=self.training,\n",
    "        )(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    features: int\n",
    "    kernel_size: Tuple[int, int] = (3, 3)\n",
    "    padding: str = \"SAME\"\n",
    "    norm: str = \"layer\"\n",
    "    training: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = ConvRelu2(\n",
    "            self.features,\n",
    "            padding=self.padding,\n",
    "            norm=self.norm,\n",
    "            training=self.training,\n",
    "        )(x)\n",
    "        x = DeConv3x3(\n",
    "            self.features // 2,\n",
    "            padding=self.padding,\n",
    "            norm=self.norm,\n",
    "            training=self.training,\n",
    "        )(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    block_size: Tuple[int, ...] = (16, 32, 64)\n",
    "    padding: str = \"SAME\"\n",
    "    norm: str = \"layer\"\n",
    "    training: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        for i, features in enumerate(self.block_size):\n",
    "            x = DownsampleBlock(\n",
    "                features=features,\n",
    "                padding=self.padding,\n",
    "                norm=self.norm,\n",
    "                training=self.training,\n",
    "            )(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    output_features: int = 3\n",
    "    block_size: Tuple[int, ...] = (16, 32, 64)\n",
    "    padding: str = \"SAME\"\n",
    "    norm: str = \"layer\"\n",
    "    training: bool = True\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "    ) -> jnp.ndarray:\n",
    "        for i, features in enumerate(self.block_size):\n",
    "            x = UpsampleBlock(\n",
    "                features=features,\n",
    "                padding=self.padding,\n",
    "                norm=self.norm,\n",
    "                training=self.training,\n",
    "            )(x)\n",
    "\n",
    "        # Final convolution to reconstruct the output\n",
    "        x = nn.Conv(\n",
    "            features=self.output_features,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=self.padding,\n",
    "        )(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "dummy_2d = jnp.ones((40, 40, 3))\n",
    "\n",
    "block_size = (12, 24, 48)\n",
    "padding = \"SAME\"\n",
    "norm = \"layer\"\n",
    "\n",
    "conv_encoder = ConvEncoder(\n",
    "    block_size=block_size,\n",
    "    padding=padding,\n",
    "    norm=norm,\n",
    ")\n",
    "conv_vars = conv_encoder.init(jax.random.PRNGKey(0), jnp.ones_like(dummy_2d))\n",
    "out = conv_encoder.apply(conv_vars, dummy_2d)\n",
    "\n",
    "conv_decoder = ConvDecoder(\n",
    "    output_features=3,\n",
    "    block_size=block_size,\n",
    "    padding=padding,\n",
    "    norm=norm,\n",
    ")\n",
    "\n",
    "conv_dec_vars = conv_decoder.init(jax.random.PRNGKey(0), jnp.ones_like(out))\n",
    "out = conv_decoder.apply(conv_dec_vars, out)\n",
    "\n",
    "assert out.shape == dummy_2d.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
