{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an RNN\n",
    "\n",
    "> An script to train a generic RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp scripts.train_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Any, List\n",
    "import pprint\n",
    "from functools import partial\n",
    "from absl import logging\n",
    "import logging as pylogging\n",
    "import hydra\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import optax\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from math import ceil\n",
    "\n",
    "from flax import traverse_util\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, orbax_utils, early_stopping\n",
    "import orbax.checkpoint as obc\n",
    "from einops import rearrange\n",
    "from physmodjax.utils.metrics import (\n",
    "    mse,\n",
    "    mae,\n",
    "    mse_relative,\n",
    "    mae_relative,\n",
    "    accumulate_metrics,\n",
    ")\n",
    "from physmodjax.utils.plot import plot_solution, plot_solution_2d\n",
    "from hydra.core.hydra_config import HydraConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_train_state(\n",
    "    model: nn.Module,\n",
    "    rng: jnp.ndarray,\n",
    "    x_shape: Tuple[\n",
    "        int, int, int, int\n",
    "    ],  # (batch_size, time_steps, grid_size, num_channels):\n",
    "    num_steps: int,  # number of training steps\n",
    "    norm: str = \"layer\",  # \"layer\" or \"batch\"\n",
    "    learning_rate: float = 1e-3,\n",
    "    grad_clip: optax.GradientTransformation = optax.clip_by_global_norm(1.0),\n",
    "    components_to_freeze: List[str] = [],\n",
    "    schedule_type: str = \"constant\",  # \"cosine\" or \"constant\"\n",
    "    debug: bool = False,  # print debug information\n",
    ") -> train_state.TrainState:\n",
    "\n",
    "    logging.info(f\"Initalizing model with shape {x_shape}.\")\n",
    "    init_key, dropout_key = jax.random.split(rng, num=2)\n",
    "    variables = model.init(\n",
    "        {\"params\": init_key, \"dropout\": dropout_key},\n",
    "        jnp.empty(shape=x_shape),\n",
    "    )\n",
    "\n",
    "    # print model parameters\n",
    "    logging.info(\n",
    "        model.tabulate(\n",
    "            init_key,\n",
    "            jnp.empty(shape=x_shape),\n",
    "            column_kwargs={\"no_wrap\": True},\n",
    "            table_kwargs={\"expand\": True},\n",
    "            console_kwargs={\"width\": 120},\n",
    "            depth=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if norm in [\"batch\"]:\n",
    "        params = variables[\"params\"]\n",
    "        batch_stats = variables[\"batch_stats\"]\n",
    "    else:\n",
    "        params = variables[\"params\"]\n",
    "\n",
    "    ssm_params = [\"nu_log\", \"theta_log\", \"gamma_log\", \"B_re\", \"B_im\", \"C_re\", \"C_im\"]\n",
    "    param_labels = traverse_util.path_aware_map(\n",
    "        lambda path, _: (\n",
    "            \"ssm\" if any(part in path for part in ssm_params) else \"regular\"\n",
    "        ),\n",
    "        params,\n",
    "    )\n",
    "\n",
    "    # freeze parameters if necessary\n",
    "    param_labels = traverse_util.path_aware_map(\n",
    "        lambda path, label: (\n",
    "            \"frozen\" if any(part in path for part in components_to_freeze) else label\n",
    "        ),\n",
    "        param_labels,\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        pp = pprint.PrettyPrinter(depth=4)\n",
    "        pp.pprint(traverse_util.flatten_dict(param_labels))\n",
    "\n",
    "    logging.info(f\"Scheduling for {num_steps} steps.\")\n",
    "    if schedule_type in [\"cosine\"]:\n",
    "        schedule_regular = optax.cosine_decay_schedule(\n",
    "            decay_steps=num_steps,\n",
    "            init_value=learning_rate,\n",
    "        )\n",
    "        schedule_ssm = optax.cosine_decay_schedule(\n",
    "            decay_steps=num_steps,\n",
    "            init_value=learning_rate / 4,\n",
    "        )\n",
    "    elif schedule_type in [\"constant\"]:\n",
    "        schedule_regular = optax.constant_schedule(learning_rate)\n",
    "        schedule_ssm = optax.constant_schedule(learning_rate / 4)\n",
    "    else:\n",
    "        raise ValueError(\"schedule_type must be 'cosine' or 'constant'\")\n",
    "\n",
    "    gradient_transform = optax.multi_transform(\n",
    "        {\n",
    "            \"ssm\": optax.adam(schedule_ssm),\n",
    "            \"regular\": optax.chain(grad_clip, optax.adamw(schedule_regular)),\n",
    "            \"frozen\": optax.set_to_zero(),\n",
    "        },\n",
    "        param_labels,\n",
    "    )\n",
    "\n",
    "    if norm in [\"layer\"]:\n",
    "\n",
    "        class TrainState(train_state.TrainState):\n",
    "            key: jax.Array\n",
    "\n",
    "        return TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=params,\n",
    "            key=dropout_key,\n",
    "            tx=gradient_transform,\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        class TrainState(train_state.TrainState):\n",
    "            key: jax.Array\n",
    "            batch_stats: Any\n",
    "\n",
    "        return TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=params,\n",
    "            tx=gradient_transform,\n",
    "            key=dropout_key,\n",
    "            batch_stats=batch_stats,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def train(\n",
    "    model_cls,\n",
    "    datamodule,\n",
    "    cfg: DictConfig,\n",
    "    checkpoint_manager: obc.CheckpointManager,\n",
    "):\n",
    "    # unpack dataloader\n",
    "    train_dataloader = datamodule.train_dataloader\n",
    "    val_dataloader = datamodule.val_dataloader\n",
    "    test_dataloader = datamodule.test_dataloader\n",
    "\n",
    "    data_shape = datamodule.get_info()\n",
    "\n",
    "    epochs_val = getattr(cfg, \"epochs_val\", 1)\n",
    "\n",
    "    # hydra multirun flag\n",
    "    hydra_multirun = (\n",
    "        hydra.core.hydra_config.HydraConfig.get().mode == hydra.types.RunMode.MULTIRUN\n",
    "    )\n",
    "\n",
    "    # generate zeros for input\n",
    "    x_shape = [datamodule.train_batch_size, *data_shape]\n",
    "\n",
    "    # initialise rng\n",
    "    rng = jax.random.PRNGKey(cfg.seed)\n",
    "\n",
    "    # Initialize optimiser, clipping and loss function\n",
    "    optimiser = hydra.utils.instantiate(cfg.optimiser)\n",
    "    grad_clip = hydra.utils.instantiate(cfg.gradient_clip)\n",
    "    loss_fn = hydra.utils.instantiate(cfg.loss)\n",
    "\n",
    "    # initialise train state\n",
    "    total_batches = datamodule.train_batch_size * len(train_dataloader)\n",
    "    state = create_train_state(\n",
    "        model_cls(n_steps=datamodule.num_steps_target_train),\n",
    "        rng,\n",
    "        x_shape,\n",
    "        num_steps=cfg.epochs * total_batches + cfg.epochs,\n",
    "        learning_rate=cfg.optimiser.learning_rate,\n",
    "        grad_clip=grad_clip,\n",
    "        components_to_freeze=cfg.frozen,\n",
    "        norm=cfg.model.norm,\n",
    "        schedule_type=cfg.schedule_type,\n",
    "    )\n",
    "\n",
    "    early_stop = early_stopping.EarlyStopping(min_delta=1e-3, patience=10)\n",
    "\n",
    "    # train step\n",
    "    @partial(jax.jit, static_argnames=(\"norm\"))\n",
    "    def train_step(\n",
    "        state: train_state.TrainState,\n",
    "        x: jnp.ndarray,  # pde solution from t=0(batch, timesteps, grid_size, channels)\n",
    "        y: jnp.ndarray,  # pde solution from t+1(batch, timesteps, grid_size, channels)\n",
    "        dropout_key: jnp.ndarray = None,\n",
    "        norm: str = \"layer\",\n",
    "    ) -> Tuple[train_state.TrainState, Dict[str, float], jnp.ndarray]:\n",
    "\n",
    "        gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        dropout_train_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
    "\n",
    "        (loss, (pred, vars)), grads = gradient_fn(\n",
    "            state.params,\n",
    "            state,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            dropout_key=dropout_train_key,\n",
    "            norm=norm,\n",
    "        )\n",
    "\n",
    "        if norm in [\"batch\"]:\n",
    "            state = state.apply_gradients(grads=grads, batch_stats=vars[\"batch_stats\"])\n",
    "        else:\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "\n",
    "        metrics = {\n",
    "            \"loss\": loss,\n",
    "            \"mse\": mse(y, pred),\n",
    "            \"mae\": mae(y, pred),\n",
    "            \"mse_rel\": mse_relative(y, pred),\n",
    "            \"mae_rel\": mae_relative(y, pred),\n",
    "        }\n",
    "        return state, metrics, pred\n",
    "\n",
    "    # val step\n",
    "    @partial(jax.jit, static_argnames=(\"model\", \"norm\"))\n",
    "    def val_step(\n",
    "        state: train_state.TrainState,\n",
    "        x: jnp.ndarray,  # pde solution from t=0(batch, timesteps, grid_size, channels)\n",
    "        y: jnp.ndarray,  # pde solution from t+1(batch, timesteps, grid_size, channels)\n",
    "        model: nn.Module,  # model to use for prediction\n",
    "        norm: str = \"layer\",\n",
    "    ):\n",
    "        if norm in [\"batch\"]:\n",
    "            pred = model.apply(\n",
    "                {\"params\": state.params, \"batch_stats\": state.batch_stats}, x\n",
    "            )\n",
    "        else:\n",
    "            pred = model.apply({\"params\": state.params}, x)\n",
    "\n",
    "        metrics = {\n",
    "            \"mse\": mse(y, pred),\n",
    "            \"mae\": mae(y, pred),\n",
    "            \"mse_rel\": mse_relative(y, pred),\n",
    "            \"mae_rel\": mae_relative(y, pred),\n",
    "        }\n",
    "        return metrics, pred\n",
    "\n",
    "    @partial(jax.jit, static_argnames=(\"model\", \"norm\"))\n",
    "    def test_step(\n",
    "        state: train_state.TrainState,\n",
    "        x: jnp.ndarray,  # pde solution (batch, timesteps, grid_size, c)\n",
    "        model: nn.Module,  # model to use for predictio\n",
    "        norm: str = \"layer\",\n",
    "    ):\n",
    "\n",
    "        # We only need the first time step for the input\n",
    "        # but the models expect a sequence with length num_steps\n",
    "        init_x = x[:, : datamodule.num_steps_input_train, ...]\n",
    "\n",
    "        def step(carry, _):\n",
    "            if norm == \"batch\":\n",
    "                pred = model.apply(\n",
    "                    {\"params\": state.params, \"batch_stats\": state.batch_stats}, carry\n",
    "                )\n",
    "            else:\n",
    "                pred = model.apply({\"params\": state.params}, carry)\n",
    "\n",
    "            return (\n",
    "                pred[\n",
    "                    :, -datamodule.num_steps_input_train :, ...\n",
    "                ],  # Update carry (with the last step) and output with the new prediction\n",
    "                pred,\n",
    "            )  # Update carry (with the last step) and output with the new prediction\n",
    "\n",
    "        # if not evenly divisible, we need to ceil the length to account the the missing input steps\n",
    "        length = ceil(x.shape[1] / datamodule.num_steps_target_train) + 1\n",
    "\n",
    "        _, preds = jax.lax.scan(step, init_x, None, length=length)\n",
    "\n",
    "        preds = rearrange(preds, \"n b s ... c -> b (n s) ... c\")\n",
    "\n",
    "        # Concatenate the initial input with the predictions\n",
    "        # WARNING for the rnn the input is always only the first time step! do not include the rest!\n",
    "        # otherwise we stack a duplicate at the start\n",
    "        full_preds = jnp.concatenate([init_x, preds], axis=1)\n",
    "\n",
    "        # we need to slice the predictions to match the input\n",
    "        full_preds = full_preds[:, : x.shape[1], ...]\n",
    "\n",
    "        metrics = {\n",
    "            \"mse\": mse(x, full_preds),\n",
    "            \"mae\": mae(x, full_preds),\n",
    "            \"mse_rel\": mse_relative(x, full_preds),\n",
    "            \"mae_rel\": mae_relative(x, full_preds),\n",
    "        }\n",
    "        return metrics, full_preds\n",
    "\n",
    "    # If hydra mode is RUN print the mode\n",
    "    if hydra_multirun:\n",
    "        logger = pylogging.getLogger(\"tqdm_logger\")\n",
    "        logger.setLevel(pylogging.INFO)\n",
    "        progress_bar = tqdm(range(1, cfg.epochs + 1), file=open(os.devnull, \"w\"))\n",
    "    else:\n",
    "        progress_bar = tqdm(range(1, cfg.epochs + 1))\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        \"\"\"Training.\"\"\"\n",
    "        train_batch_metrics = []\n",
    "        for x, y in train_dataloader:\n",
    "\n",
    "            state, metrics, pred = train_step(\n",
    "                state,\n",
    "                x=x,\n",
    "                y=y,\n",
    "                dropout_key=rng,\n",
    "                norm=cfg.model.norm,\n",
    "            )\n",
    "            train_batch_metrics.append(metrics)\n",
    "        train_batch_metrics = accumulate_metrics(train_batch_metrics)\n",
    "\n",
    "        # Validation\n",
    "        if ((epoch - 1) % epochs_val == 0) or (epoch == cfg.epochs):\n",
    "            \"\"\"Validation.\"\"\"\n",
    "            val_batch_metrics = []\n",
    "            for x, y in val_dataloader:\n",
    "\n",
    "                metrics, pred = val_step(\n",
    "                    state,\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    model=model_cls(\n",
    "                        training=False,\n",
    "                        n_steps=datamodule.num_steps_target_val,\n",
    "                    ),  # use model with dropout off\n",
    "                    norm=cfg.model.norm,\n",
    "                )\n",
    "                val_batch_metrics.append(metrics)\n",
    "            val_batch_metrics = accumulate_metrics(val_batch_metrics)\n",
    "            early_stop = early_stop.update(val_batch_metrics[\"mae_rel\"])\n",
    "\n",
    "            test_batch_metrics = []\n",
    "            for test_x in test_dataloader:\n",
    "                # the test step is always autoregressive\n",
    "                metrics, test_pred = test_step(\n",
    "                    state,\n",
    "                    x=test_x,\n",
    "                    model=model_cls(\n",
    "                        training=False,\n",
    "                        n_steps=datamodule.num_steps_target_train,\n",
    "                    ),  # use model with dropout off\n",
    "                    norm=cfg.model.norm,\n",
    "                )\n",
    "                test_batch_metrics.append(metrics)\n",
    "            test_batch_metrics = accumulate_metrics(test_batch_metrics)\n",
    "\n",
    "            if early_stop.should_stop:\n",
    "                logging.info(\"Met early stopping criteria, breaking...\")\n",
    "                break\n",
    "\n",
    "            # Log Metrics to Weights & Biases\n",
    "            metrics_to_log = {\n",
    "                \"train/loss\": float(train_batch_metrics[\"loss\"]),\n",
    "                \"train/mse\": float(train_batch_metrics[\"mse\"]),\n",
    "                \"train/mae\": float(train_batch_metrics[\"mae\"]),\n",
    "                \"train/mse_rel\": float(train_batch_metrics[\"mse_rel\"]),\n",
    "                \"train/mae_rel\": float(train_batch_metrics[\"mae_rel\"]),\n",
    "                \"val/mse\": float(val_batch_metrics[\"mse\"]),\n",
    "                \"val/mae\": float(val_batch_metrics[\"mae\"]),\n",
    "                \"val/mse_rel\": float(val_batch_metrics[\"mse_rel\"]),\n",
    "                \"val/mae_rel\": float(val_batch_metrics[\"mae_rel\"]),\n",
    "                \"test/mse_rel\": float(test_batch_metrics[\"mse_rel\"]),\n",
    "                \"test/mae_rel\": float(test_batch_metrics[\"mae_rel\"]),\n",
    "            }\n",
    "\n",
    "            wandb.log(\n",
    "                metrics_to_log,\n",
    "                step=epoch,\n",
    "            )\n",
    "\n",
    "            # log images\n",
    "            single_y = y[0, ..., 0]  # single entry, only last channel\n",
    "            single_pred = pred[0, ..., 0]  # single entry, only last channel\n",
    "\n",
    "            if len(data_shape) == 4:\n",
    "                fig = plot_solution_2d(\n",
    "                    gt=single_y,\n",
    "                    pred=single_pred,\n",
    "                    # ar_pred=ar_pred[..., 0] if datamodule.mode == \"many_to_many\" else None,\n",
    "                )\n",
    "            elif len(data_shape) == 3:\n",
    "                fig = plot_solution(\n",
    "                    gt=single_y,\n",
    "                    pred=single_pred,\n",
    "                    ar_gt=test_x[0, ..., 0],  # single entry, only last channel\n",
    "                    ar_pred=test_pred[0, ..., 0],  # single entry, only last channel\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid training data shape\")\n",
    "\n",
    "            images = wandb.Image(\n",
    "                fig,\n",
    "            )\n",
    "            plt.close(fig)\n",
    "            wandb.log({\"end train epoch\": images})\n",
    "\n",
    "            # Save checkpoint\n",
    "            checkpoint_manager.save(\n",
    "                step=epoch,\n",
    "                args=obc.args.Composite(\n",
    "                    state=obc.args.PyTreeSave(state),\n",
    "                ),\n",
    "                metrics=metrics_to_log,\n",
    "            )\n",
    "        else:\n",
    "            # # Log Metrics to Weights & Biases\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train/loss\": train_batch_metrics[\"loss\"],\n",
    "                    \"train/mse\": train_batch_metrics[\"mse\"],\n",
    "                    \"train/mae\": train_batch_metrics[\"mae\"],\n",
    "                    \"train/mse_rel\": train_batch_metrics[\"mse_rel\"],\n",
    "                    \"train/mae_rel\": train_batch_metrics[\"mae_rel\"],\n",
    "                },\n",
    "                step=epoch,\n",
    "            )\n",
    "        progress_bar.set_postfix({\"loss\": float(train_batch_metrics[\"loss\"])})\n",
    "\n",
    "        if hydra_multirun:\n",
    "            logger.info(str(progress_bar))\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"../../conf\", config_name=\"train_rnn\")\n",
    "def train_rnn(cfg: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Train RNN model\n",
    "    \"\"\"\n",
    "    OmegaConf.register_new_resolver(\n",
    "        \"eval\",\n",
    "        eval,\n",
    "        replace=True,\n",
    "    )\n",
    "    \n",
    "    logging.debug(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "    jax.config.update(\"jax_platform_name\", cfg.jax.platform_name)\n",
    "    logging.debug(\"jax devices: \", jax.devices())\n",
    "\n",
    "    # Set matplotlib backend to Agg when running on cluster\n",
    "    matplotlib.use(\"Agg\")\n",
    "\n",
    "    # Initialise logging\n",
    "    output_dir = Path(HydraConfig.get().run.dir).absolute()\n",
    "\n",
    "    wandb.require(\"core\")\n",
    "    run = wandb.init(\n",
    "        dir=output_dir,\n",
    "        config=OmegaConf.to_container(\n",
    "            cfg,\n",
    "            resolve=True,\n",
    "            throw_on_missing=False,\n",
    "        ),\n",
    "        **cfg.wandb,\n",
    "    )\n",
    "\n",
    "    model_cls = hydra.utils.instantiate(cfg.model)\n",
    "    datamodule = hydra.utils.instantiate(cfg.datamodule)\n",
    "\n",
    "    # Log data info\n",
    "    wandb.config.update({\"output_dir\": output_dir})\n",
    "    wandb.config.update({\"data_info\": datamodule.get_info()})\n",
    "    wandb.config.update(\n",
    "        {\"data_std\": datamodule.std if hasattr(datamodule, \"std\") else None}\n",
    "    )\n",
    "    wandb.config.update(\n",
    "        {\"data_mean\": datamodule.mean if hasattr(datamodule, \"mean\") else None}\n",
    "    )\n",
    "\n",
    "    options = obc.CheckpointManagerOptions(\n",
    "        max_to_keep=1,\n",
    "        create=True,\n",
    "        best_fn=lambda x: float(x[\"val/mse\"]),\n",
    "        best_mode=\"min\",\n",
    "    )\n",
    "\n",
    "    with obc.CheckpointManager(\n",
    "        directory=Path(output_dir) / \"checkpoints\",\n",
    "        options=options,\n",
    "        item_handlers={\"state\": obc.PyTreeCheckpointHandler()},\n",
    "    ) as checkpoint_manager:\n",
    "\n",
    "        state = train(\n",
    "            model_cls=model_cls,\n",
    "            datamodule=datamodule,\n",
    "            cfg=cfg,\n",
    "            checkpoint_manager=checkpoint_manager,\n",
    "        )\n",
    "\n",
    "        checkpoint_manager.wait_until_finished()\n",
    "\n",
    "    logging.info(\n",
    "        f\"Checkpoint best step {checkpoint_manager.best_step()}, number of steps: {checkpoint_manager.all_steps()}\"\n",
    "    )\n",
    "\n",
    "    # Save model to wandb\n",
    "    artifact = wandb.Artifact(\n",
    "        name=f\"checkpoints_{wandb.run.id}\",\n",
    "        type=\"model\",\n",
    "    )\n",
    "    artifact.add_dir(checkpoint_manager.directory, name=\"checkpoints\")\n",
    "    run.log_artifact(artifact)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  _target_: physmodjax.fno.rnn.BatchFNORNN\n",
      "  hidden_channels: 4\n",
      "  grid_size: 101\n",
      "  n_spectral_layers: 2\n",
      "  out_channels: 2\n",
      "datamodule:\n",
      "  _target_: physmodjax.scripts.dataset_generation.DirectoryDataModule\n",
      "  batch_size: 1\n",
      "  data_directory: data/test\n",
      "jax:\n",
      "  platform_name: cpu\n",
      "  preallocate_gpu_memory: false\n",
      "optimiser:\n",
      "  _target_: optax.adam\n",
      "  learning_rate: 0.001\n",
      "gradient_clip:\n",
      "  _target_: optax.clip_by_global_norm\n",
      "  max_norm: 1.0\n",
      "seed: 3407\n",
      "epochs: 1\n",
      "wandb:\n",
      "  project: physmodjax\n",
      "  entity: iir-modal\n",
      "  group: rnn-test\n",
      "  job_type: train\n",
      "  name: null\n",
      "project: physmodjax\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702859671.726977   71570 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n",
      "[2023-12-18 00:34:31,800][jax._src.xla_bridge][INFO] - Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA\n",
      "[2023-12-18 00:34:31,801][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "jax devices:  [CpuDevice(id=0)]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:33<00:00, 33.96s/it]\n",
      "[2023-12-18 00:35:08,662][absl][INFO] - OCDBT is initialized successfully.\n",
      "[2023-12-18 00:35:08,662][absl][INFO] - Saving item to /home/carlos/projects/physmodjax/outputs/2023-12-18/00-34-31/checkpoints.\n",
      "[2023-12-18 00:35:08,687][absl][INFO] - Renaming /home/carlos/projects/physmodjax/outputs/2023-12-18/00-34-31/checkpoints.orbax-checkpoint-tmp-1702859708663140 to /home/carlos/projects/physmodjax/outputs/2023-12-18/00-34-31/checkpoints\n",
      "[2023-12-18 00:35:08,687][absl][INFO] - Finished saving checkpoint to `/home/carlos/projects/physmodjax/outputs/2023-12-18/00-34-31/checkpoints`.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make a ROOT_DIR global variable that can be used anywhere to run commands reproducibly. Maybe force hydra to always run there?\n",
    "!cd ../.. ; env HYDRA_FULL_ERROR=1 WANDB_MODE=disabled train_rnn +experiment=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
