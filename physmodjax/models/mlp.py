# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models/mlp.ipynb.

# %% auto 0
__all__ = ['MLP']

# %% ../../nbs/models/mlp.ipynb 2
from typing import Sequence
import flax.linen as nn
import jax.numpy as jnp

# %% ../../nbs/models/mlp.ipynb 3
class MLP(nn.Module):
    """
    MLP with SELU activation and LeCun normal initialization.
    """

    hidden_channels: Sequence[int]  # number of hidden channels
    activation: nn.Module = nn.selu
    kernel_init: nn.initializers.Initializer = nn.initializers.lecun_normal()
    use_bias: bool = True
    layer_norm: bool = False

    @nn.compact
    def __call__(
        self,
        x: jnp.ndarray,
    ) -> jnp.ndarray:
        for channels in self.hidden_channels:
            x = nn.Dense(
                features=channels,
                kernel_init=self.kernel_init,
                use_bias=self.use_bias,
            )(x)
            if channels != self.hidden_channels[-1]:
                if self.layer_norm:
                    x = nn.LayerNorm()(x)
                x = self.activation(x)
        return x
