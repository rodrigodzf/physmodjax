# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models/recurrent.ipynb.

# %% auto 0
__all__ = ['BatchedDeepRNN', 'LRUDynamicsVarying', 'DeepRNN']

# %% ../../nbs/models/recurrent.ipynb 2
import jax
import jax.numpy as jnp
import flax.linen as nn
from functools import partial
from einops import rearrange

# %% ../../nbs/models/recurrent.ipynb 5
from physmodjax.models.ssm import (
    theta_init,
    nu_init,
)

# %% ../../nbs/models/recurrent.ipynb 6
from .ssm import LRUDynamics

# %% ../../nbs/models/recurrent.ipynb 8
class LRUDynamicsVarying(LRUDynamics):

    model: nn.Module  # model to process the linear state

    def setup(self):
        super().setup()

    def __call__(
        self,
        x: jnp.ndarray,  # initial complex state flattened (d_hidden,) complex
        steps: int,  # number of steps to advance
    ) -> jnp.ndarray:  # advanced state (steps, d_hidden) complex

        x = super().__call__(x, steps)
        x_hat = self.model(x.real**2 + x.imag**2)
        x_hat = x_hat[..., : self.d_hidden] + 1j * x_hat[..., self.d_hidden :]
        x = x * x_hat
        return x

# %% ../../nbs/models/recurrent.ipynb 12
class DeepRNN(nn.Module):
    """
    A deep RNN model that applies a RNN cell over the last dimension of the input.
    Works with nn.GRUCell, nn.RNNCell, nn.SimpleCell, nn.MGUCell.
    """

    d_model: int
    d_vars: int
    n_layers: int
    cell: nn.Module
    training: bool = True
    norm: str = "layer"

    def setup(self):

        # scan does the same thing as nn.RNN (unrolls the over the time dimension)
        self.first_layer = nn.RNN(
            self.cell(features=self.d_model * self.d_vars),
        )

        self.layers = [
            nn.RNN(
                self.cell(features=self.d_model * self.d_vars),
            )
            for _ in range(self.n_layers)
        ]

    def __call__(
        self,
        x0: jnp.ndarray,  # (W, C) # initial state
        x: jnp.ndarray,  # (T, W, C) # empty state
    ) -> jnp.ndarray:  # (T, W, C) # advanced state
        # the rnn works over the last dimension
        # we need to reshape the input to (T, d_model * C)
        x0 = rearrange(x0, "w c -> (w c)")
        x = rearrange(x, "t w c -> t (w c)")
        x = self.first_layer(x, initial_carry=x0)
        for layer in self.layers:
            x = layer(x)
        return rearrange(x, "t (w c) -> t w c", w=self.d_model, c=self.d_vars)


BatchedDeepRNN = nn.vmap(
    DeepRNN,
    in_axes=0,
    out_axes=0,
    variable_axes={"params": None},
    split_rngs={"params": False},
    axis_name="batch",
)
